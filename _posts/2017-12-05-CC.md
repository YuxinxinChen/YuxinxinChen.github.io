---
layout: post
title: "CC Report"
data: 2017-12-05
tags: [CC, Groute, Gunrock]
comments: true
share: false
---

##  Async: Groute
### Algorithm Groute
To compute connected components (CC), Groute uses an algorithm called Adaptive CC, which is a variant of Soman's CC algorithm. Adaptive CC differs from Soman's CC algorithm on one main point: instead of running multiple passes of non-atomic hooks (each pass hooks each edge once) followed by a multi pointer jumping, it runs one pass of atomic hooks on all edges followed by a multi pointer jumping. The choice between Adaptive CC algorithm and Soman's CC algorithm boils down to a choice between using atomic operation and using synchronization (which can be costly due to both synchronization mechanism and load unbalancing). If we run CC on single CPU or GPU, we can simply run one pass of atomic hooks and one multi pointer jumping, then the program ends. If we run adaptive CC on multiple devices, we can divide the edge list into several segments; each device gets one segment. Each device runs one atomic hook on its local edges to compute the local connected component information. then the local connected components are merged across all devices.

Here is the pseudo code of Adaptive CC:
```c
for each node u do in parallel
        Parent(u) = u
end for
s = 2|E|/|V|
let E_1 â€¦.E_s be s distinct subsets of E
for each E_i
        for each edge (u,v) in E_i do in parallel
                AtomicHook((u,v))
        end for
        for all v in V do in parallel
                MultiJump(v)
        end for
end for
```

```c
AtomicHook((u,v)):
while Parent(u)!=Parent(v) do
        H = max{Parent(u), Parent(v)}
        L = min{Parent(u), Parent(v)}
        lock Parent(H)
                if Parent(H) = H: then
                        Parent(H) = L
                        return
                else
                        u = Parent(H)
                        v = L
                end if
        end lock
end while
```

```c
MultiJump(v):
while Parent(Parent(v))!=Parent(v) do
	Parent(v) = Parent(Parent(v))
end while
```
### Groute Pipeline
Another important feature in Adaptive CC is the use of pipelining. When data movement happens in Groute, Groute can divide the data into chunks and send out the data chunk by chunk, so the receiver of the data can start performing computation as soon as it receives the first chunk of data. The downside of pipelining is that because we pipeline the data movement and data processing by the size of chunk, it may result the device under utilization if the chunk size is small. However in today's architecture, the data movement is considered more expensive generally, so hiding the data movement by pipelining may be beneficial (???). Also we can always maximize the chunk size to saturate the network (network or PCIe or NVLink). In CC, because we assume all data is on the device at the begining, the communication between devices mostly consists of connected components information transfer between GPUs, which can be largly hidden by pipelining.

### Complexity
Based on above, it is easy to understand this diagram:
![Groute](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_cc.png)

In a multi-GPU system, each GPU will run the grey block (???) in parallel. So in theory, we only need run one pass of atomic hooks (over all local edges) on each GPU in parallel. However within the grey block (???), there is a local loop h which equals to the number edges per GPU divided by chunk size. The reason we divide the edges into many chunks and process them in a loop is that: 1) we can pipeline the data offloading to GPU thus hiding the data movement. 2) empirically, we make the chunk size equal to the number of vertices, so when we run atomic hook, there will be only |V| atomic operations on a memory space of size |V|, reducing the atomic operation contention.

After each GPU finishes running atomic-hook on all its local edges, Groute pipelines the data movement to merge the cc information across all GPUs

```bash
Let |V| be number of vertices in the graph G
Let |E| be number of edges in the graph G
Let IC be the chunk size when offloading the edges to GPUs. By default IC = |V|
Let RC be the chunk size when sending local CC information right before merge stage. By default RC = |V|/2
Let N be the number of GPUs in the multi-GPU system
Let t_at be the time used to run atomic hook on a chunk of edges of size IC
Let t_merg be the time used to merge a chunk of CC information of size RC
Let t_com be the communication time before merge which is not hidden by Groute's pipelining
```
For number of total operations done in the system:

```bash
Total_op = O(|E|+|E||V|/IC+N|V|) = O(|E|+N|V|)
```
For the runtime:

```bash
runtime = t_at*(|E|/(N|V|)) + t_com + t_merg*2N
```
Empirically, the runtime is largely occupied by the first part: t_at\*(|E|/(N|V|)). t_at is the time taken to run atomic hooks on a chunk of edges of size |V| (as it is the default IC value). t_merg is also the time taken to run atomic hooks on a chunk of edges of size |V|. Thus, t_at and t_merg should be on the same scale, however, t_at is multiplied by (|E|/(N|V|) which can be fairly large. In a larger system, we can expect N to be large. We also expect t_com to be large, because of slow data tranfer. However a lot of the communication time overlaps with computation time due to pipelining, which reduces the value of t_com.

## Sync: Gunrock
### Algorithm Gunrock

Gunrock uses Soman's CC algorithm. On single device, Soman's CC will loop on all edges, until all edges are hooked onto the spanning tree. Then it does a multi pointer jumping. The Soman CC algorithm ends. A good thing about Soman's hook is that it is not atomic. The race condition is benigh but the algorithm requires more iterations of hook to form the spanning tree. Between each iteration of hook, we need to synchronize across all edges. If we run Soman's CC on multi device system, we divid the edges into part and each device gets some. Then each device to iterations of non-atomic hook on assigned edges, then all device converges locally, they exchange the cc information by sending to all other device and start the non-atomic hook when it receives the cc infor from all other devices. The device repetes those untill converge globally. (And this is somehow weird to me, after each GPU converge locally, they can simplely send the cc information to GPU0, and GPU0 does iterations of non-atomic hook on the receive the data to converge, and the algorihm ends. Why there are global iterations? I might think gunrock might have done some redudent work. There is no need to send cc information to all other device)

Here is the psudo code of Soman's CC

```c
while trees are not star-trees or there are edges between star-trees
        for each edge (u,v) do in parallel
                hook(u,v)
        end for
        for each node u do in parallel
                multi-pointer-jumping(u)
        end for
end while
```

```c
hook(u,v)
if Parent(u) > Parent(v)
        Parent(u) = Parent(v)
else Parent(v) = Parent(u)
end if
```

```c
multi-pointer-jumping(u)
while Parent(Parent(u))!=Parent(u)
        Parent(u) = Parent(Parent(u))
end while
```
### Gunrock

Gunrock is a BSP model based framework. It is data-centric framework. So the algorithm progresses by applying operations on frontier then we gets a new frontier. So the choice of Soman's CC is natural. Gunrock can map the hook operation onto its operators and updates the frontier in a loop until it converges. In multi-GPU system, each GPU keeps a local frontier, to reduce commincation crossing GPU, each GPU works on its local frontier until there is no work can be done, then GPUs exchange information with each others and use the information to update local frontier until converge globally. 

There is no synchronization across all GPU in Gunrock. However the implication synchronization comes from the communication. Because the local frontier is synchronized, then it always need to wait until all information are received from other devices to start local computation on local frontier. In larger system, the communiation crossing the system can be hurt.

### Complexity

From above illustration, it is easy to understand CC's diagram on Gunrock:
![Gunrock](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/gunrock_cc.png)

So in multi-GPU system, each GPU gets part of edges of the graph and works on those edges until converges locally. Then GPUs communicate cc information by sending its own cc information to all other GPUs. After each GPU receives from all other GPUs, it incoperates the information and starts non-atomic hooks & multi pointer jumping works again on local frontier untill cc information converges globally. 

```bash
Define |V| is number of vertices in the graph G
Define |E| is number of edges in the graph G
Define I is the outer iteration in the above diagram
Define i is the inter iteration within each device in the above diagram
Define C is the average operations done to merge the cc information from all other devices each outer iteration
Define the t_mem is the time used to communication between each outer iterations
Define the t_non is the time used to finish a iteration of non-atomic hook on frontier
```

For number of total operations done in the system:
```bash
Total_op = O(((|E|+N|V|)*i + C)*I)
```
For the runtime:

```bash
runtime = (t_non*i + t_mem)*I
```

Usually, I is relatively small and i is large. In the dataset we have tested, the runtime is dominated by t_non\*i\*I. In a larger system, we could expect t_mem can be hurt.

## Comparison

The advantages and disadvantages of Synchronization framework represeted by Gunrock are:
Advantages:
1) No atomic operations
2) Easy implementation
Disadvantages:
1) Synchronization

Then advantages and disadvantages  of Asynchronization framework represented by Groute are:
Advantages:
1) No large scope of synchronization
2) Piplined communciation and computation
Disadvantages:
1) Atomic operations
2) Nasty implementation

Piplined communcation and computation quite depends on the communcation volume, bandwith and computation volum and this feature is not avaliable on Gunrock, so we won't compare this feature between two framwork, but we do think the piplined communication and computation can help alleviate communication cost. Different level of difficulty for implementation may depend on the softerware infrastruction. For now, the infrastruction supporting synchronization framework is better. For example, GPU is a very BSP model friendly device.

Then we will compare mainly the cost of atomic work VS cost of synchronization in the following comparison part on different types of graph.
```bash
Define T_local is the average time used on local work and data tranfer within each other iteration in Gunrock framwork
Define S is the cost of synchronization in Gunrock framework
Define I is the number of outer iteration in Gunrock framework
Define T_ato is the runtime of CC in Groute framework. 

Gunrock:
runtime = (T_local + S)I 
If we don't change graph, then we can get a T_local and I value by running multiple experiment on Gunrock. 

Groute:
runtime = T_ato = t_ato + t_mem + t_merg
t_ato and t_merg depend on the speed of atomic operation, so if we increase cost of atomic operation, t_ato, t_merg will increasing and thus T_ato. 

### Comparison on Kron dataset
WE run Kron dataset on Gunrock and Groute respectively:
Grunrock:
total_runtime = 88.5351 ms
I  = 5
T_local = 14.3317 ms
S = 3.376 ms

Groute:
T_ato = 29.11845 ms
```
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/grout_prof_kron.png)
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/grout_overlap_kron.png)
Because t_ato + t_merg takes more than 90% of T_ato, and the computation on each GPU is well overlapped, we use T_ato to reflex the influence of atomic operation on runtime

We can get this:
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kron.png)
This is the break even graph of Groute and Gunrock as a function of synchronization cost and computation time of atomic-hook. So the left side the the blue line in the above graph is the region you should use Groute, and the right side of the blue line is the region you should use Gunrock. The red cross point is the point where we get by running Gunrock and Groute on luigi system with 4 GPUs.



