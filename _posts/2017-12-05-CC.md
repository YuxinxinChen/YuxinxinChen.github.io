---
layout: post
title: "CC Report"
data: 2017-12-05
tags: [CC, Groute, Gunrock]
comments: true
share: false
---

##  Async: Groute
### Algorithm Groute
Groute use Adaptive connected component algorithm which is a variance of Soman connected component algorithm. It differs from Soman's CC algorithm on: Instead of running multiple time of non-atomic hooks followed by a multi pointer jumping, it runs one atomic hook followed by a multi pointer jumping. We think the choice between Adaptive CC algorithm and Soman CC algorithm is a choice between using atomic operation and using synchronization (which can be costly from both synchronization mechanism and load unbalancing). So if we run CC on single CPU or GPU, we can actually run one atomic-hook and one multi pointer jumping, then the program ends. If we run adaptive CC on multiple devices, we can divide the edge list into several parts; each device gets some. Each device does atomic-hook locally and gets a local connected components inforation, then merge the local connected components across all devices.

Here is the psudo code of Adaptive CC
```c
for each node u do in parallel
        Parent(u) = u
end for
s = 2|E|/|V|
let E_1 â€¦.E_s be s distinct subsets of E
for each E_i
        for each edge (u,v) in E_i do in parallel
                AtomicHook((u,v))
        end for
        for all v in V do in parallel
                MultiJump(v)
        end for
end for
```

```c
AtomicHook((u,v)):
while Parent(u)!=Parent(v) do
        H = max{Parent(u), Parent(v)}
        L = min{Parent(u), Parent(v)}
        lock Parent(H)
                if Parent(H) = H: then
                        Parent(H) = L
                        return
                else
                        u = Parent(H)
                        v = L
                end if
        end lock
end while
```

```c
MultiJump(v):
while Parent(Parent(v))!=Parent(v) do
	Parent(v) = Parent(Parent(v))
end while
```
### Groute Pipeline
Another important feature in Groute CC is the use of pipelining. So when data movement happens in Groute, Groute can divide the data into chunks and send out the data chunk by chunk, so the receiver of the data can start to arrange the computation work as long as it recevices the first chunk of data. The downside of this pipeline is that because we pipeline the data movement and data processing by the size of chunk, it may result the device under utilization if the chunk size is small. However in today's architecture, the data movement is considered more expensive generally, so hiding the data movement by do pipeline may be beneficial. Also we will always maximize the chunk size to saturate the network (network or PCIe or NVLink). In CC, because we assume all data is on the device at the begining, then the communication btween devices is the connected components information transfer between GPUs, which can be largly hidden by pipelining.

### Complexity
Based on above, it is easy to understand this diagram:
![Groute](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_cc.png)

So in multi-GPU system, each GPU will run the grey block in parallel. So in theory, we only need run one atomic-hook on each GPU in parallel. However within the grey block, there is a locl loop h which equals to the number edges per GPU divided by chunk size. The reason we divide the edges into many chunks and process them in a loop is that: 1) we can pipeline the data offloading to GPU (But because we assume all data are on GPUs at beginning, we don't pipeline to offload the data to GPUs) thus hiding the data movement. 2) empirically, we make the chunk size equals to the number of vertices, then when we run atomic-hook, there will be only |V| atomic operations on a memory space of size |V|, reducing the atomic operation contention.

After each GPU finishing its atomic-hook on assigned edges, Groute pipelines the data movement to merge the cc information across all GPUs 

```bash
Define |V| is number of vertices in the graph G
Define |E| is number of edges in the graph G
Define IC is the chunk size offloading the edges to GPUs and as default IC = |V|
Define RC is the chunk size sending crossing the GPUs before merge stage. As default RC = |V|/2
Define there are N GPUs avaliable in the multi-GPU system
Define t_at is the time used to run atomic-hook on a chunk size (IC) edgelist data
Define t_merg is the time used to merge on a chunk size (RC) cc data
Define t_com is the time on communiation before merge which can not be hidden by Groute pipelining
```
For number of total operations done in the system:

```bash
Total_op = O(|E|+N|V|)
```
For the runtime:

```bash
runtime = t_at*(|E|/(N|V|)) + t_com + t_merg*2N
```
So the runtime is largely occupied by the first part: t_at\*(|E|/(N|V|)). t_at is the time running atomic-hook on |V| chunk size of data. t_merg is also the time running atomic-hook on |V| chunk size of data. t_at and t_merg should on the same scale, however, t_at is multiplied by (|E|/(N|V|) which can be fairly large. In a larger system, we can expect N is large and t_com because of slow data tranfer. However pipeling can alleviate the pain.

## Sync: Gunrock
### Algorithm Gunrock

Gunrock uses Soman's CC algorithm. On single device, Soman's CC will loop on all edges, until all edges are hooked onto the spanning tree. Then it does a multi pointer jumping. The Soman CC algorithm ends. A good thing about Soman's hook is that it is not atomic. The race condition is benigh but the algorithm requires more iterations of hook to form the spanning tree. Between each iteration of hook, we need to synchronize across all edges. If we run Soman's CC on multi device system, we divid the edges into part and each device gets some. Then each device to iterations of non-atomic hook on assigned edges, then all device converges locally, they exchange the cc information by sending to all other device and start the non-atomic hook when it receives the cc infor from all other devices. The device repetes those untill converge globally. (And this is somehow weird to me, after each GPU converge locally, they can simplely send the cc information to GPU0, and GPU0 does iterations of non-atomic hook on the receive the data to converge, and the algorihm ends. Why there are global iterations? I might think gunrock might have done some redudent work. There is no need to send cc information to all other device)

Here is the psudo code of Soman's CC

```c
while trees are not star-trees or there are edges between star-trees
        for each edge (u,v) do in parallel
                hook(u,v)
        end for
        for each node u do in parallel
                multi-pointer-jumping(u)
        end for
end while
```

```c
hook(u,v)
if Parent(u) > Parent(v)
        Parent(u) = Parent(v)
else Parent(v) = Parent(u)
end if
```

```c
multi-pointer-jumping(u)
while Parent(Parent(u))!=Parent(u)
        Parent(u) = Parent(Parent(u))
end while
```
### Gunrock

Gunrock is a BSP model based framework. It is data-centric framework. So the algorithm progresses by applying operations on frontier then we gets a new frontier. So the choice of Soman's CC is natural. Gunrock can map the hook operation onto its operators and updates the frontier in a loop until it converges. In multi-GPU system, each GPU keeps a local frontier, to reduce commincation crossing GPU, each GPU works on its local frontier until there is no work can be done, then GPUs exchange information with each others and use the information to update local frontier until converge globally. 

There is no synchronization across all GPU in Gunrock. However the implication synchronization comes from the communication. Because the local frontier is synchronized, then it always need to wait until all information are received from other devices to start local computation on local frontier. In larger system, the communiation crossing the system can be hurt.

### Complexity

From above illustration, it is easy to understand CC's diagram on Gunrock:
![Gunrock](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/gunrock_cc.png)

So in multi-GPU system, each GPU gets part of edges of the graph and works on those edges until converges locally. Then GPUs communicate cc information by sending its own cc information to all other GPUs. After each GPU receives from all other GPUs, it incoperates the information and starts non-atomic hooks & multi pointer jumping works again on local frontier untill cc information converges globally. 

```bash
Define |V| is number of vertices in the graph G
Define |E| is number of edges in the graph G
Define I is the outer iteration in the above diagram
Define i is the inter iteration within each device in the above diagram
Define C is the average operations done to merge the cc information from all other devices each outer iteration
Define the t_mem is the time used to communication between each outer iterations
Define the t_non is the time used to finish a iteration of non-atomic hook on frontier
```

For number of total operations done in the system:
```bash
Total_op = O(((|E|+N|V|)*i + C)*I)
```
For the runtime:

```bash
runtime = (t_non*i + t_mem)*I
```

Usually, I is relatively small and i is large. In the dataset we have tested, the runtime is dominated by t_non\*i\*I. In a larger system, we could expect t_mem can be hurt.

## Comparison

