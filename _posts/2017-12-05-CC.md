---
layout: post
title: "CC Report"
data: 2017-12-05
tags: [CC, Groute, Gunrock]
comments: true
share: false
---

##  Async: Groute
### Algorithm Groute
Groute use Adaptive connected component algorithm which is a variance of Soman connected component algorithm. It differs from Soman's CC algorithm on: Instead of running multiple time of non-atomic hooks followed by a multi pointer jumping, it runs one atomic hook followed by a multi pointer jumping. We think the choice between Adaptive CC algorithm and Soman CC algorithm is a choice between using atomic operation and using synchronization (which can be costly from both synchronization mechanism and load unbalancing). So if we run CC on single CPU or GPU, we can actually run one atomic-hook and one multi pointer jumping, then the program ends. If we run adaptive CC on multiple devices, we can divide the edge list into several parts; each device gets some. Each device does atomic-hook locally and gets a local connected components inforation, then merge the local connected components across all devices.

Here is the psudo code of Adaptive CC
```c
for each node u do in parallel
        Parent(u) = u
end for
s = 2|E|/|V|
let E_1 â€¦.E_s be s distinct subsets of E
for each E_i
        for each edge (u,v) in E_i do in parallel
                AtomicHook((u,v))
        end for
        for all v in V do in parallel
                MultiJump(v)
        end for
end for
```

```c
AtomicHook((u,v)):
while Parent(u)!=Parent(v) do
        H = max{Parent(u), Parent(v)}
        L = min{Parent(u), Parent(v)}
        lock Parent(H)
                if Parent(H) = H: then
                        Parent(H) = L
                        return
                else
                        u = Parent(H)
                        v = L
                end if
        end lock
end while
```

```c
MultiJump(v):
while Parent(Parent(v))!=Parent(v) do
	Parent(v) = Parent(Parent(v))
end while
```
### Groute Pipeline
Another important feature in Groute CC is the use of pipelining. So when data movement happens in Groute, Groute can divide the data into chunks and send out the data chunk by chunk, so the receiver of the data can start to arrange the computation work as long as it recevices the first chunk of data. The downside of this pipeline is that because we pipeline the data movement and data processing by the size of chunk, it may result the device under utilization if the chunk size is small. However in today's architecture, the data movement is considered more expensive generally, so hiding the data movement by do pipeline may be beneficial. Also we will always maximize the chunk size to saturate the network (network or PCIe or NVLink). In CC, because we assume all data is on the device at the begining, then the communication btween devices is the connected components information transfer between GPUs, which can be largly hidden by pipelining.

### Complexity
Based on above, it is easy to understand this diagram:
![Groute](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_cc.png)

So in multi-GPU system, each GPU will run the grey block in parallel. So in theory, we only need run one atomic-hook on each GPU in parallel. However within the grey block, there is a locl loop h which equals to the number edges per GPU divided by chunk size. The reason we divide the edges into many chunks and process them in a loop is that: 1) we can pipeline the data offloading to GPU (But because we assume all data are on GPUs at beginning, we don't pipeline to offload the data to GPUs) thus hiding the data movement. 2) empirically, we make the chunk size equals to the number of vertices, then when we run atomic-hook, there will be only |V| atomic operations on a memory space of size |V|, reducing the atomic operation contention.

After each GPU finishing its atomic-hook on assigned edges, Groute pipelines the data movement to merge the cc information across all GPUs 

```bash
Define |V| is number of vertices in the graph G
Define |E| is number of edges in the graph G
Define IC is the chunk size offloading the edges to GPUs and as default IC = |V|
Define RC is the chunk size sending crossing the GPUs before merge stage. As default RC = |V|/2
Define there are N GPUs avaliable in the multi-GPU system
Define t_at is the time used to run atomic-hook on a chunk size (IC) edgelist data
Define t_merg is the time used to merge on a chunk size (RC) cc data
Define t_com is the time on communiation before merge which can not be hidden by Groute pipelining
```
For number of total operations done in the system:

```bash
Total_op = O(|E|+N|V|)
```
For the runtime:

```bash
runtime = t_at*(|E|/(N|V|)) + t_com + t_merg*2N
```
So the runtime is largely occupied by the first part: t_at\*(|E|/(N|V|)). t_at is the time running atomic-hook on |V| chunk size of data. t_merg is also the time running atomic-hook on |V| chunk size of data. t_at and t_merg should on the same scale, however, t_at is multiplied by (|E|/(N|V|) which can be fairly large. In a larger system, we can expect N is large and t_com because of slow data tranfer. However pipeling can alleviate the pain.

## Sync: Gunrock

## Comparison

