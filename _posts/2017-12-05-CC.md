---
layout: post
title: "CC Report"
data: 2017-12-05
tags: [CC, Groute, Gunrock]
comments: true
share: false
---

##  Async: Groute
### Algorithm Groute
To compute connected components (CC), Groute uses an algorithm called Adaptive CC, which is a variant of Soman's CC algorithm. Adaptive CC differs from Soman's CC algorithm on one main point: instead of running multiple passes of non-atomic hooks (each pass hooks each edge once) followed by a multi pointer jumping, it runs one pass of atomic hooks on all edges followed by a multi pointer jumping. The choice between Adaptive CC algorithm and Soman's CC algorithm boils down to a choice between using atomic operation and using synchronization (which can be costly due to both synchronization mechanism and load unbalancing). If we run CC on single CPU or GPU, we can simply run one pass of atomic hooks and one multi pointer jumping, then the program ends. If we run adaptive CC on multiple devices, we can divide the edge list into several segments; each device gets one segment. Each device runs one atomic hook on its local edges to compute the local connected component information. then the local connected components are merged across all devices.

Here is the pseudo code of Adaptive CC:
```c
for each node u do in parallel
        Parent(u) = u
end for
s = 2|E|/|V|
let E_1 â€¦.E_s be s distinct subsets of E
for each E_i
        for each edge (u,v) in E_i do in parallel
                AtomicHook((u,v))
        end for
        for all v in V do in parallel
                MultiJump(v)
        end for
end for
```

```c
AtomicHook((u,v)):
while Parent(u)!=Parent(v) do
        H = max{Parent(u), Parent(v)}
        L = min{Parent(u), Parent(v)}
        lock Parent(H)
                if Parent(H) = H: then
                        Parent(H) = L
                        return
                else
                        u = Parent(H)
                        v = L
                end if
        end lock
end while
```

```c
MultiJump(v):
while Parent(Parent(v))!=Parent(v) do
	Parent(v) = Parent(Parent(v))
end while
```
### Groute Pipeline
Another important feature in Adaptive CC is the use of pipelining. When data movement happens in Groute, Groute can divide the data into chunks and send out the data chunk by chunk, so the receiver of the data can start performing computation as soon as it receives the first chunk of data. The downside of pipelining is that because we pipeline the data movement and data processing by the size of chunk, it may result the device under utilization if the chunk size is small. However in today's architecture, the data movement is considered more expensive generally, so hiding the data movement by pipelining may be beneficial (???). Also we can always maximize the chunk size to saturate the network (network or PCIe or NVLink). In CC, because we assume all data is on the device at the begining, the communication between devices mostly consists of connected components information transfer between GPUs, which can be largly hidden by pipelining.

### Complexity
Based on above, it is easy to understand this diagram:
![Groute](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_cc.png)

In a multi-GPU system, each GPU will run the grey block (???) in parallel. So in theory, we only need run one pass of atomic hooks (over all local edges) on each GPU in parallel. However within the grey block (???), there is a local loop h which equals to the number edges per GPU divided by chunk size. The reason we divide the edges into many chunks and process them in a loop is that: 1) we can pipeline the data offloading to GPU thus hiding the data movement. 2) empirically, we make the chunk size equal to the number of vertices, so when we run atomic hook, there will be only |V| atomic operations on a memory space of size |V|, reducing the atomic operation contention.

After each GPU finishes running atomic-hook on all its local edges, Groute pipelines the data movement to merge the cc information across all GPUs

```bash
Let |V| be number of vertices in the graph G
Let |E| be number of edges in the graph G
Let IC be the chunk size when offloading the edges to GPUs. By default IC = |V|
Let RC be the chunk size when sending local CC information right before merge stage. By default RC = |V|/2
Let N be the number of GPUs in the multi-GPU system
Let t_at be the time used to run atomic hook on a chunk of edges of size IC
Let t_merg be the time used to merge a chunk of CC information of size RC
Let t_com be the communication time before merge which is not hidden by Groute's pipelining
```
For number of total operations done in the system:

```bash
Total_op = O(|E|+|E||V|/IC+N|V|) = O(|E|+N|V|)
```
For the runtime:

```bash
runtime = t_at*(|E|/(N|V|)) + t_com + t_merg*2N
```
Empirically, the runtime is largely occupied by the first part: t_at\*(|E|/(N|V|)). t_at is the time taken to run atomic hooks on a chunk of edges of size |V| (as it is the default IC value). t_merg is also the time taken to run atomic hooks on a chunk of edges of size |V|. Thus, t_at and t_merg should be on the same scale, however, t_at is multiplied by (|E|/(N|V|) which can be fairly large. In a larger system, we can expect N to be large. We also expect t_com to be large, because of slow data tranfer. However a lot of the communication time overlaps with computation time due to pipelining, which reduces the value of t_com.

## Sync: Gunrock
### Algorithm Gunrock
Gunrock uses Soman's CC algorithm. On a single device, Soman's CC make multiple passes over all edges. In each pass, hook is called on every edge. After each pass of hooks, a global synchronization happens. The algorithm continues until all edges are hooked onto the spanning tree. Next, the algorithm does a multi pointer jumping. Then the algorithm ends. The good thing about Soman's hook is that it is not atomic. The race condition is benign but some computation results can get overwritten, so the algorithm requires multiple passes of hooks to form the spanning tree. Between each iteration of hook, we need to synchronize across all edges. 

If we run Soman's CC on multi device system, we divide the edges into partitions and each device gets some. Then each device repeatedly runs non-atomic hook on its local edges until convergence. After that, they exchange the local CC information by sending to all other device.

Each device then waits until it receives the local CC information of all other devices and merges these with its own local CC information. Then it starts the next iterations of non-atomic hooks. This is repeated until global convergence

Here is the psudo code of Soman's CC

```c
while trees are not level-one or there are edges between level-one trees
        for each edge (u,v) do in parallel
                hook(u,v)
        end for
        for each node u do in parallel
                multi-pointer-jumping(u)
        end for
end while
```

```c
hook(u,v)
if Parent(u) > Parent(v)
        Parent(u) = Parent(v)
else Parent(v) = Parent(u)
end if
```

```c
multi-pointer-jumping(u)
while Parent(Parent(u))!=Parent(u)
        Parent(u) = Parent(Parent(u))
end while
```
### Gunrock
Gunrock is a BSP model based framework. It is data-centric framework. So the algorithm progresses by applying operations on frontier then we gets a new frontier. So the choice of Soman's CC is natural. Gunrock can map the hook operation onto its operators and updates the frontier in a loop until it converges. In multi-GPU system, each GPU keeps a local frontier, to reduce cross-GPU communication, each GPU works on its local frontier until no more work can be done, then the GPUs exchange information with each other and use the information to update local frontier until global convergence.

There is no explicit global synchronization across all GPU in Gunrock. However the implicit global synchronization comes from the need of each GPU to receive the computational results of all other GPUs before proceeding with the next iteration of local computation. In a large system, the communication crossing the system can also hurt.

### Complexity

The following diagram illustrates Soman's CC algorithm implemented in Gunrock:
![Gunrock](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/gunrock_cc.png)

In a multi-GPU system, each GPU gets part of edges of the graph and works on those edges until it converges locally. Then each GPU sends its own CC information to all other GPUs. After each GPU receives from all other GPUs, it merges the information and starts non-atomic hooks & multi pointer jumping works again on local frontier until CC information converges globally.

```bash
Let |V| be number of vertices in the graph G
Let |E| be number of edges in the graph G
Let I be the number of outer iterations in the above diagram
Let i be the number of inner iterations on each device in the above diagram
Let C be the average number of operations done to merge the CC information from all other devices each outer iteration
Let t_mem be the average communication time between two outer iterations
Let t_non be the average time to run one non-atomic hook on each edge on a single GPU
```

For number of total operations done in the system:
```bash
Total_op = O(((|E|+N|V|)*i + C)*I)
```
For the runtime:

```bash
runtime = (t_non*i + t_mem)*I
```
Usually, I is relatively small and i is large. In the dataset we have tested, the runtime is dominated by t_non\*i\*I. In a large system, we could expect t_mem can hurt.

## Comparison

The advantages and disadvantages of the Synchronous framework (Gunrock) are: 
Advantages:
1) No atomic operations
2) Easy implementation

Disadvantages:
1) Synchronization

Then advantages and disadvantages of the Asynchronous framework (Groute) are: 
Advantages:
1) No large scope of synchronization
2) Piplined communication and computation

Disadvantages:
1) Atomic operations
2) Nasty implementation

The benefit offered by pipelining communication and computation depends heavily on the communication volume, bandwith and computation volume. Gunrock does not do pipelining, so we won't compare this feature between two framworks, but we do think the pipelined communication and computation can help alleviate communication cost. Different levels of difficulty for implementation may depend on the software infrastructure. For now, the infrastructure supporting synchronous framework is better. For example, GPU is a very BSP model friendly device.

In the rest of this document, we will focus on the comparison between the cost of atomic operations VS the cost of synchronization. We will also provide some empirical comparisons on several graphs.

```bash
Let T_local be the average time of one outer iteration in Gunrock (this includes the computation and data transfer times)
Let S be the cost of synchronization in Gunrock (the wait time due to load-unbalance across GPUs and cost of the send/receive mechanism)
Let I be the number of outer iterations in Gunrock
Let T_atomic be the runtime of CC in Groute

Gunrock:
runtime = (T_local + S)I 
For a fixed graph, then we can get a roughly constant value of T_local and I by taking the average across multiple experiment runs on Gunrock. 

Groute:
runtime = T_atomic = t_ato + t_mem + t_merg
t_ato and t_merg depend on the speed of atomic operation, so if the cost of atomic operation increases, t_ato, t_merg will increase and thus T_atomic will also increase.
```

### Comparison on Kron dataset
Kron21
```bash
|V| = 2097152
|E| = 182081864
Ave_degree = 86.8233985900879
```
We run Kron dataset on Gunrock and Groute respectively:

Grunrock:
```bash
total_runtime = 88.5351 ms
I  = 5
T_local = 14.3317 ms
S = 3.376 ms
```
Groute:
```bash
T_atomic = 29.11845 ms
```

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/grout_prof_kron.png)
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/grout_overlap_kron.png)
Because t_ato + t_merg takes more than 90% of T_atomic, and the computation on each GPU is well overlapped, we use T_atomic to reflect the influence of atomic operation on runtime

We can get this:

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kron.png)

In the above plot, the line represents the break even point between Groute and Gunrock as a function of synchronization cost and cost of atomic-hook. 
If the (synchronization cost, atomic operation cost) falls on the left side of the line, then one should use Groute, conversely, if the point falls to the right side of the line, then one should use Gunrock. The red cross is a point obtained from running Gunrock and Groute on Luigi system with 4 GPUs used on Kron graph

### Comparison on Soc-LiveJournal1 dataset
Soc-liveJournal:
```bash
|V| = 4847571
|E| = 68993773
Ave_degree = 14.2326482685865
```
We run Soc-LiveJournal1 on Gunrock and Goute respectively:

Gunrock:
```bash
total_runtime = 127.0466 ms
I = 5
T_local = 21.5119 ms
S = 3.89042 ms
```
Groute:
```bash
T_atomic = 21.913150 ms
```
```bash
==17336== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   47.15%  1.26274s       340  3.7139ms  896.42us  4.9384ms  [CUDA memcpy HtoD]
                   36.94%  989.18ms       300  3.2973ms  444.07us  13.111ms  void HookHighToLowAtomic<groute::graphs::dev::EdgeList>(groute::graphs::dev::Irregular<int>, groute::graphs::dev::EdgeList)
                    6.80%  181.97ms        60  3.0328ms  1.0510ms  11.218ms  [CUDA memcpy DtoH]
                    4.83%  129.46ms       340  380.75us  304.80us  446.66us  MultiJumpCompress(groute::graphs::dev::Irregular<int>)
                    3.02%  80.873ms        80  1.0109ms  948.23us  1.7476ms  [CUDA memcpy PtoP]
                    0.89%  23.949ms       120  199.58us  178.37us  218.85us  void HookHighToLowAtomic<groute::graphs::dev::Tree>(groute::graphs::dev::Irregular<int>, groute::graphs::dev::Tree)
                    0.36%  9.6807ms        80  121.01us  120.64us  121.60us  InitParents(groute::graphs::dev::Irregular<int>)
```
Because T_atomic dominates about 80% of total runtime.

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/soc.png)

In the above plot, the line represents the break even point between Groute and Gunrock as a function of synchronization cost and cost of atomic-hook.
If the (synchronization cost, atomic operation cost) falls on the left side of the line, then one should use Groute, conversely, if the point falls to the right side of the line, then one should use Gunrock. The red cross is a point obtained from running Gunrock and Groute on Luigi system with 4 GPUs used on Soc-liveJournal1 graph

### Comparison on Road-USA dataset
Road-USA:
```bash
|V| = 23947347
|E| = 57708624
Ave_degree = 2.40981282811829
```
We run Soc-LiveJournal1 on Gunrock and Goute respectively:

Gunrock:
```bash
total_runtime = 827.2967 ms
I = 9
T_local = 61.7115 ms
S =  30.2103 ms
```
Groute:
```bash
T_atomic = 171.213350
```
```bash
==19532== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   37.03%  1.06261s       160  6.6413ms  1.2680ms  15.267ms  [CUDA memcpy HtoD]
                   26.25%  753.39ms       100  7.5339ms  1.2706ms  62.525ms  [CUDA memcpy DtoH]
                   19.38%  556.24ms        80  6.9530ms  5.0685ms  8.1536ms  void HookHighToLowAtomic<groute::graphs::dev::EdgeList>(groute::graphs::dev::Irregular<int>, groute::graphs::dev::EdgeList)
                    8.14%  233.73ms       200  1.1686ms  778.60us  2.1992ms  MultiJumpCompress(groute::graphs::dev::Irregular<int>)
                    3.37%  96.824ms        80  1.2103ms  1.1683ms  2.5619ms  [CUDA memcpy PtoP]
                    2.90%  83.178ms        80  1.0397ms  883.11us  1.2411ms  void Hook<groute::graphs::dev::EdgeList, bool=1>(groute::graphs::dev::Irregular<int>, groute::graphs::dev::EdgeList)
                    2.10%  60.138ms       160  375.86us  275.27us  597.45us  void HookHighToLowAtomic<groute::graphs::dev::Tree>(groute::graphs::dev::Irregular<int>, groute::graphs::dev::Tree)
                    0.83%  23.687ms        80  296.08us  148.90us  589.77us  InitParents(groute::graphs::dev::Irregular<int>)
```
T_atomic dominates about 58% of total runtime

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/usa.png)

In the above plot, the line represents the break even point between Groute and Gunrock as a function of synchronization cost and cost of atomic-hook.
If the (synchronization cost, atomic operation cost) falls on the left side of the line, then one should use Groute, conversely, if the point falls to the right side of the line, then one should use Gunrock. The red cross is a point obtained from running Gunrock and Groute on Luigi system with 4 GPUs used on road-USA graph


