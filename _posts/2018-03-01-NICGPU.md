
layout: post
title: "Notes on GPU"
data: 2017-12-10
tags: [reading notes, NIC, GPU]
comments: true
share: false
---

Generally speaking, making GPU comminicate with remote CPU/GPU via networking, we should consider two important aspects and how they are done in current state of art: 
1. Data movement
2. Control path
So when we ask the GPU commincate, or more specifically sending or recieveing data to or from remote nodes, we first need to tell the HCA to send or receive data and where is the send data if it is send, when to send the data. We need to know if the data arrive if it is recieve. When we say: send!. The data is send from GPU to NIC. From this process, there are two path: control path which set everything up and prepare the buttom you can press to send the data; the actual data movement which the data movement is happening.

In the following reviews, all of them, the data movement happens directly from NIC and GPU, bypassing the copy from GPU to CPU and from CPU to NIC due to the use of GPUDirect RDMA (expose GPU memory region to be directly accessible on the PCIe bus via a BAR window). However, some of them also offload the control path to GPU, some of them ask CPU to set up everything and GPU to press send buttom, some of them put the control path all on CPU side.

##GPUrdma
GPUrdma is a GPU-side library for performing remote direct memory access across network directly from GPU kernel, completely bypassing CPU, pusing both data and control on GPU. 
####GPUDirect RDMA
From Kepler, GPUDirect RDMA relies on the standard PCIe capabilities to perform peer-to-peer DMA across PCIe devices without CPU involvement. For example, one GPU may read from the memory of another GPU over PCIe, as long as the memory-mapped I/O region exposing the memory of the first is mapped into the address sapce of the second. These capabilities, combined with the GPUDirect RDMA API for GPU memory translations and mappings, enable any PCIe device to access GPU memory in the same way it accesses the CPU memory. 
####Infiniband and HCA
HCA works as a DMA to free the data path from OS involvement. In order to use the network, a process ask the HCA to create a Queue Pair (QP) which has a send work queue and a receive work queue. The send work queue allows the process to be the initiator of transport operations by writing into the entries a Work Queue Element (WQE) describing the send task. The receive work queue allows the process to be a target of a transport operation by writing into the entries a Receive Queue Element (RQE). Each QP has it ID. Each QP, there is a corresponding Completion Queue Element (CQE) which delivers a Compiletion nitification to the initiating process with the status of the operation. We can poll on the CQE to see if the operation is done. So when two processes create a transport connection by instruction the HCA to pair their local QP, as the below shows. So all the information are set up, paired WQE, RQE, send queue, receive queue, place to see send status, and sent data inforamtion is stored in WQE. Where is the buttom we coud press to send? This process is called ring the doorbell. There is a doorbell register in HCA for each QP. The write to the doorbell notifies the HCA to handle the next work request by advancing to the next WQE in the QP.
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/HCA.jpg)

To achive above steps, there is one more thing needs to be set up. The doorbell is on HCA's memory, but the data buffer which will be packed and send out onto network, the QP are on CPU memory. So the data buffer, QP need to be mapped onto HCA's virtual memory system via an operation called Memory registration and a user process performs Infiniband transport operations using VERBs which is the API of Infiniband protocal. The doorbell on HCA also need to be mapped onto CPU's virtual address space and accessed by MMIO as the below figure shows:
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/cpuHCA.jpg)

To let the GPU communicate with HCA, we can map GPU's data buffer and QP onto HCA's virtual memory space and map HCA's doorbell onto GPU, then same process happens between CPU and HCA happens between GPU and HCA. GPU fills the information by writing into WQE and poll on CQE, and ring the doorbell when the time to send. As the below figure shows:
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/GPUHCA.jpg)

###GPUrdma design decision
1. For single GPU thread, it keep sending requests without worrying about its completion and it only poll on CQ when the send queue is full or all the messages are sent out. Only ring the doorbell when the QP is full.
2. Increases the number of QPs to leverage multiple GPU threads to concurrently create multiple transfer requests. It is actually a widely used techniq to optimize CPU tranfer performance. Then there are some decision the programmer need to make to cater the application:
	1. QP per warp: Good: Packet ordering rules of the RDMA transport protocal force the HCA to serialize the processing of packets that belong to the same QP. Increasing the number of QPs allows the HCA to process different packets from different QPs in parallel.
			Bad: high memory comsuption
	2. QP per threadblock: Good: less memory comsuption
			       Bad: less parallel on HCA.

3. Where to put the QP/CQ. If the QP/CQ on CPU, it is faster for HCA to access but slower for GPU to write into. If it is on GPU, it is faster for GPU to write into, but slower for HCA to access (peer to peer access). But the experiments show it is better to put both on GPU memory.


###Limitation
It is ok:
	send_A
	wait_send_A_complete
	send_B
	wait_send_B_complete

On remote machine:
	get_A
	Operate_on_A
	get_B
	Operate_on_B

If we send A and B asynchronousely:
	send_A
	send_B
	wait_send_A_complete
	wait_send_B_complete
The remote machine might receive B first then A, and do Operate_on_A on B and do Operate_on_B on A. 

Furthermore, the scalability could be poor, since each threadblock need a QP/CQ and when the number of GPU increase, this number could become very large.

##Comparison
So the most widely use model is that the data transfer calls are invoked by the CPU, while ensuring that the GPU kernel that generates the data terminates before the tranfer start by synchronizing. The problems with this scheme are that:
1. The codes are forced to be programmed in a bulk-synchronouse way, then makes overlapping computation and communications challenging. 
2. Synchronizing between kernel execution and network operations is complicated since GPUs and NICs expose different synchronization mechanisms and event interfaces, and even more challenging when executing multiple kernels and I/O calls in parallel to achieve pipelinging.
3. Since to tranfer data, the kernel need to end. If each kernel runs long enough, it may justify the kernel launch overhead. But if the kernel is small then the kernel launch overhead can be significant. Also since we have to end the kernel to communicate, we lose many opportunities to use shared memory to achieve better performance.
4. The messages to be transferred to other machines must be accumulated in internal GPU buffers during the kernel execution. This in turn increase the kernel memory consumption or constrains the amount of data kernel may process each time.
5. GPUrdma enable a more natural application design in which computations and I/O are interleaved, and the I/O challs are performed directly from GPU code. As a result, rather than terminate the kernel for performing I/O, we can execute long running threadblocks throughout the entire lifetime of the application in a persistent kernel style.



