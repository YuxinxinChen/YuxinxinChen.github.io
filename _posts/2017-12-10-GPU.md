
layout: post
title: "Notes on GPU"
data: 2017-12-10
tags: [reading notes, GPU]
comments: true
share: false
---

### Brief History of GPU Architecture before Fermi

The first GPU invented by NVIDIA in 1999. From 1999 to 2017, GPU became more general from more specific, become more specific from more general back and forth. From 2003, there are many cool kids hack into the GPU, making it little bit more programmable. Later, by using high-level shading languages such as DirectX, OpenGL, they exploited the GPU for non-graphical application. However, it was fairly ackward since everything has be expressed in terms of vertex coordinates, textures and shader programs. Everything is constrained. NVIDIA is a smart company, they made the first programmed GPU with CUDA enabled: G80. Then GPU computing become easier and signified broader application support. In 2008, NVIDIA introduced the second generation unified architecture GT200: increasing the number of streaming processor cores from 128 to 240. Each processor register file was doubled in size, allowing a greater number of theads to execute on-chip at any given time. Hardward memory access coalescing was added to improve memory acess efficiency and double precision floating point support was added for HPC applications.

### Fermi

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi1.png)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi2.png)

When doing CUDA programming, programmer organizes theads in thread blocks and grid of thread blocks. The GPU instantiates a kernel program on a grid of parallel thread blocks. Each thread within a thread block executes an instance of the kernel, and has a thread ID within its thread block, program counter, registers, per-thread private memory, inputs, and output results. 
A thread block is a set of concurrently executing threads that can cooperate among themselves through barrier synchronization and shared memory. A thread block has a block ID within its grid.

A grid is an array of thread blocks that execute the same kernel, read inputs from global memory, write results to global memory, and synchronize between dependent kernel calls. In the CUDA parallel programming model, each thread has a per-thread private memory space used for register spills, function calls, and C automatic array variables. Each thread block has a per-Block shared memory space used for inter-thread communication, data sharing, and result sharing in parallel algorithms. Grids of thread blocks share results in Global Memory space after kernel-wide global synchronization. 

CUDA's programming hierarchy of thread maps to the hierarchy of processors on GPU; a GPU executes one or more kernel grids; a streaming multiprocessor (SM) executes one or more thread blocks; and CUDA cores and other execution units in the SM execute threads 

So to understand how it works, let's assume we have two application A and B. Application A is composed of two kernels: Kernel1 and Kernel2. Application B is composed of three kernels: Kernel3, Kernel4 and Kernel5. Assume Kernel1 and Kernel2 can use up all the threads the GPU can provide but Kernel1 and Kernel2 are independent. Kernel3, Kernel4 and Kernel5 can only use 1/3 of the computing resource of the GPU and they are independent.

What happening follows:
1. The GigaThread distributor does context switching between application A and B like a CPU doing hyperthreading, where each program receives a time slice of the processor's resources. Say at this time slice, application A is running. Then GigaThread schedules thread blocks to SMs, because Kernel1 and Kernel2 are all mapped to a grid of threads using up all the computing resource, so without specifying multi-stream, only one kernel runs on the SMs. However if we specify 2 mutli-stream, Kernel1 and Kernel2 get scheduled simultaneously and each gets some number of SMs to run on ??? (However, I am not sure if Fermi allow multi-stream). If say, at this time slice, application B is runnign. Then GigaThread schedulers each kernel with 5 SMs, 5 SMs and 6 SMs and they can run concurrently. However in Fermi, kernels of different application context have to run sequentially with fast contect switching.

+ So different kernels of same program can run concurrently on different SMs. For Fermi, it can run concurrent kernels up to 16 (Fermi has 16 SMs)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi3.png)

2. Each SM gets the thread block from GigaThread. It schedules threads in warps. Each SM features two warp schedulers and two instruction dispatch units, allowing two warps to be issued and executed concurrently. Fermi's warp schedular doesn't need to check for dependencies from within the instruction stream as the figure above shows. So if you programlly define the thread block size very large, then it will scheduler several wraps to finish the work. Why using the dual-issue model? In the above Fermi SM figure, it only has 16 CUDA cores, but in GF100, the number of CUDA cores increases to 32, in GF104, the number of CUDA cores increase to 48, while the number of SFUs increases to 8. For Fermi, a warp is executed over 2 (or more) clocks of the CUDA cores – 16 threads are processed and then the other 16 threads in that warp are processed. For full SM utilization, all threads must be running the same instruction at the same time. For these reasons a SM is internally divided up in to a number of execution units that a single dispatch unit can dispatch work to:

> 16 CUDA cores (#1)
> 16 CUDA cores (#2)
> 16 Load/Store Units
> 16 Interpolation SFUs
> 4 Special Function SFUs
> 4 Texture Units

With 2 warp scheduler/dispatch unit pairs in each SM, GF100 can utilize at most 2 of 6 execution units at any given time. It’s also because of the SM being divided up like this that it was possible for NVIDIA to add to it. GF104 in comparison has the following:

> 16 CUDA cores (#1)
> 16 CUDA cores (#2)
> 16 CUDA cores (#3)
> 16 Load/Store Units
> 16 Interpolation SFUs
> 8 Special Function SFUs
> 8 Texture Units

This gives GF104 a total of 7 execution units, the core of which are the 3 blocks of 16 CUDA cores.
With 2 warp schedulers, GF100 could put all 32 CUDA cores to use if it had 2 warps where both required the use of CUDA cores. With GF104 this gets more complex since there are now 3 blocks of CUDA cores but still only 2 warp schedulers. So how does NVIDIA feed 3 blocks of CUDA cores with only 2 warp schedulers? They go superscalar.

In a nutshell, superscalar execution is a method of extracting Instruction Level Parallelism from a thread. If the next instruction in a thread is not dependent on the previous instruction, it can be issued to an execution unit for completion at the same time as the instruction preceding it. There are several ways to extract ILP from a workload, with superscalar operation being something that modern CPUs have used as far back as the original Pentium to improve performance. For NVIDIA however this is new – they were previously unable to use ILP and instead focused on Thread Level Parallelism (TLP) to ensure that there were enough warps to keep a GPU occupied.

+ Each SM at each time slice is assigned one or more thread blocks from one or more kernels
+ SM scheduler selects two warps and issues one instruction from each warp to a group of 16 cores, 16 load/store units or 4 SFUs. The two warps can be from different thread blocks then might be from different kernels?? There is no requirement for the warp schedulers to select two warps from the same thread block.
+ When issuing two warps, the scheduler doesn't check the instruction dependency between those two warps.
+ Within a warp, it uses superscalar to fully utlize the GPU. 

Other features Fermi has:
1. Improve Double Precision Performance
2. Add ECC support
3. Add Cache hierarchy: configurable and more L1/shared memory + L2 cache
4. Faster Context Switching and Atomic Operations 
5. More CUDA cores per SM and more SMs
6. Unified Address Space with C++ support
7. Improved performance through prediction
8. Dual overlapped memory transfer engines
9. Out of order thread block execution

Usefull links: NVIDIA Fermi Compute Architecture Whitepaper <http://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf> and GF104 Reviews <https://www.anandtech.com/show/3809/nvidias-geforce-gtx-460-the-200-king/2> and Stackoverflow Question <https://stackoverflow.com/questions/10460742/how-do-cuda-blocks-warps-threads-map-onto-cuda-cores>

### Kepler 

Kepler is more targeting on HPC market while Maxwell is more on graphics market. First a Kepler GK110 chip block diagram:

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kepler1.png)

