
layout: post
title: "Notes on GPU"
data: 2017-12-10
tags: [reading notes, GPU]
comments: true
share: false
---

### Brief History of GPU Architecture before Fermi

The first GPU invented by NVIDIA in 1999. From 1999 to 2017, GPU became more general from more specific, become more specific from more general back and forth. From 2003, there are many cool kids hack into the GPU, making it little bit more programmable. Later, by using high-level shading languages such as DirectX, OpenGL, they exploited the GPU for non-graphical application. However, it was fairly ackward since everything has be expressed in terms of vertex coordinates, textures and shader programs. Everything is constrained. NVIDIA is a smart company, they made the first programmed GPU with CUDA enabled: G80. Then GPU computing become easier and signified broader application support. In 2008, NVIDIA introduced the second generation unified architecture GT200: increasing the number of streaming processor cores from 128 to 240. Each processor register file was doubled in size, allowing a greater number of theads to execute on-chip at any given time. Hardward memory access coalescing was added to improve memory acess efficiency and double precision floating point support was added for HPC applications.

### Fermi

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi1.png)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi2.png)

When doing CUDA programming, programmer organizes theads in thread blocks and grid of thread blocks. The GPU instantiates a kernel program on a grid of parallel thread blocks. Each thread within a thread block executes an instance of the kernel, and has a thread ID within its thread block, program counter, registers, per-thread private memory, inputs, and output results. 
A thread block is a set of concurrently executing threads that can cooperate among themselves through barrier synchronization and shared memory. A thread block has a block ID within its grid.

A grid is an array of thread blocks that execute the same kernel, read inputs from global memory, write results to global memory, and synchronize between dependent kernel calls. In the CUDA parallel programming model, each thread has a per-thread private memory space used for register spills, function calls, and C automatic array variables. Each thread block has a per-Block shared memory space used for inter-thread communication, data sharing, and result sharing in parallel algorithms. Grids of thread blocks share results in Global Memory space after kernel-wide global synchronization. 

CUDA's programming hierarchy of thread maps to the hierarchy of processors on GPU; a GPU executes one or more kernel grids; a streaming multiprocessor (SM) executes one or more thread blocks; and CUDA cores and other execution units in the SM execute threads 

So to understand how it works, let's assume we have two application A and B. Application A is composed of two kernels: Kernel1 and Kernel2. Application B is composed of three kernels: Kernel3, Kernel4 and Kernel5. Assume Kernel1 and Kernel2 can use up all the threads the GPU can provide but Kernel1 and Kernel2 are independent. Kernel3, Kernel4 and Kernel5 can only use 1/3 of the computing resource of the GPU and they are independent.

What happening follows:
1. The GigaThread distributor does context switching between application A and B like a CPU doing hyperthreading, where each program receives a time slice of the processor's resources. Say at this time slice, application A is running. Then GigaThread schedules thread blocks to SMs, because Kernel1 and Kernel2 are all mapped to a grid of threads using up all the computing resource, so without specifying multi-stream, only one kernel runs on the SMs. However if we specify 2 mutli-stream, Kernel1 and Kernel2 get scheduled simultaneously and each gets some number of SMs to run on but will require more time to finish each kernels. If say, at this time slice, application B is runnign. Then GigaThread might schedulers each kernel with 5 SMs, 5 SMs and 6 SMs and they can run concurrently. However in Fermi, kernels of different application context have to run sequentially with fast contect switching.

+ So different kernels of same program can run concurrently on different SMs. For Fermi, it can run concurrent kernels up to 16 (Fermi has 16 SMs)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi3.png)

2. Each SM gets the thread block from GigaThread. It schedules threads in warps. Each SM features two warp schedulers and two instruction dispatch units, allowing two warps to be issued and executed concurrently. Fermi's warp schedular doesn't need to check for dependencies from within the instruction stream as the figure above shows. So if you programlly define the thread block size very large, then it will scheduler several wraps to finish the work. Why using the dual-issue model? In the above Fermi SM figure, it only has 16 CUDA cores, but in GF100, the number of CUDA cores increases to 32, in GF104, the number of CUDA cores increase to 48, while the number of SFUs increases to 8. For Fermi, a warp is executed over 2 (or more) clocks of the CUDA cores – 16 threads are processed and then the other 16 threads in that warp are processed. For full SM utilization, all threads must be running the same instruction at the same time. For these reasons a SM is internally divided up in to a number of execution units that a single dispatch unit can dispatch work to:

> 16 CUDA cores (#1)

> 16 CUDA cores (#2)

> 16 Load/Store Units

> 16 Interpolation SFUs

> 4 Special Function SFUs

> 4 Texture Units

With 2 warp scheduler/dispatch unit pairs in each SM, GF100 can utilize at most 2 of 6 execution units at any given time. It’s also because of the SM being divided up like this that it was possible for NVIDIA to add to it. GF104 in comparison has the following:

> 16 CUDA cores (#1)

> 16 CUDA cores (#2)

> 16 CUDA cores (#3)

> 16 Load/Store Units

> 16 Interpolation SFUs

> 8 Special Function SFUs

> 8 Texture Units

This gives GF104 a total of 7 execution units, the core of which are the 3 blocks of 16 CUDA cores.
With 2 warp schedulers, GF100 could put all 32 CUDA cores to use if it had 2 warps where both required the use of CUDA cores. With GF104 this gets more complex since there are now 3 blocks of CUDA cores but still only 2 warp schedulers. So how does NVIDIA feed 3 blocks of CUDA cores with only 2 warp schedulers? They go superscalar.

In a nutshell, superscalar execution is a method of extracting Instruction Level Parallelism from a thread. If the next instruction in a thread is not dependent on the previous instruction, it can be issued to an execution unit for completion at the same time as the instruction preceding it. There are several ways to extract ILP from a workload, with superscalar operation being something that modern CPUs have used as far back as the original Pentium to improve performance. For NVIDIA however this is new – they were previously unable to use ILP and instead focused on Thread Level Parallelism (TLP) to ensure that there were enough warps to keep a GPU occupied.

+ Each SM at each time slice is assigned one or more thread blocks from one or more kernels
+ SM scheduler selects two warps and issues one instruction from each warp to a group of 16 cores, 16 load/store units or 4 SFUs. The two warps can be from different thread blocks then might be from different kernels?? There is no requirement for the warp schedulers to select two warps from the same thread block.
+ When issuing two warps, the scheduler doesn't check the instruction dependency between those two warps.
+ Within a warp, it uses superscalar to fully utlize the GPU. 

Other features Fermi has:
1. Improve Double Precision Performance
2. Add ECC support
3. Add Cache hierarchy: configurable and more L1/shared memory + L2 cache
4. Faster Context Switching and Atomic Operations 
5. More CUDA cores per SM and more SMs
6. Unified Address Space with C++ support
7. Improved performance through prediction
8. Dual overlapped memory transfer engines
9. Out of order thread block execution

Usefull links: NVIDIA Fermi Compute Architecture Whitepaper <http://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf> and GF104 Reviews <https://www.anandtech.com/show/3809/nvidias-geforce-gtx-460-the-200-king/2> and Stackoverflow Question <https://stackoverflow.com/questions/10460742/how-do-cuda-blocks-warps-threads-map-onto-cuda-cores>

### Kepler 

Kepler is more targeting on HPC market while Maxwell is more on graphics market. First a Kepler GK110 chip block diagram:

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kepler1.png)

Weird layout with 15 SMX (not SM), still 6 memory controller, L2 Cache shared by all SMX. But new SMX is much much powerfull. Let's see an overall infor about kepler below: each SMX, it can support more threads (add CUDA cores) and more thread blocks (more schedulers and dispatch units) and more registers (each thread need register to put temp data thendo fast context switch. Then more registers can prevent spill and support more threads). 


![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kepler2.png)

A detailed look of SMX:

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kepler3.png)

The first look is much much more CUDA cores and DP units and load/store units and special function unites on one SMX and much more registers and texture units. Also there are 4 warp schedulers and 8 dispatch units. It add 48KB read-only data cache.

* 192 CUDA cores, 64 double-precision units, 32 SFU and 32 L/S units.
* 255 register per thread.
* Add read-only data cache. 
* 4 warp schedulers, 8 dispatch units, allowing 4 warps to run concurrently.

So after adding so many cores, to keep energy efficiency, kepler use many less powerful cores (lower clock rate but fully pipelined floating-point and integer arithmetic units).
Because it now has 4 warp schedulers and 8 dispatch units, it allows four warps to be issued and executed concurrently.
For warp scheduler also be optimized in other aspects:
1. Register scoreboarding for long latency operations (texture and load)
2. Inter-warp scheduling decisions (pick the best warp to go next among eligible candidates)
3. Thread block level scheduling 
4. Instead having a complex hardware stage to prevent data hazards(A multi-port register scoreboard keeps track of any registers that are not ready with valid data, and a dependency checker blocl analyzes register usage across a multitude of fully decoded warp instructions against the scoreboard, to determine which are eligible to issue), Kepler determine the dependency at compile time, thus replacing several power-hungry blocks.

I think an important feature Kepler has is DYNAMIC PARALLELISM:
So basically, it add grid management unit (GMU), which collects the task generately dynamically on GPU and schedule them appropriatly. In kepler, a grid can be launched from the CPU just as was the case with Fermi: GigaThread work distributor distribute the thread blocks to SMXs, but CUDA-created grids will be managed and prioritized by grid management unit then passed into Gigathread work distributor which send to SMX for execution. 
The GigaThread work distributor in Kepler holds grids that are ready to dispatch, and it is able to dispatch 32 active grids, which is double the capacity of the Fermi GigaThread work distributor. The Kepler GigaThread work distributor communicates with the GMU via a bidirectional link that allows the GMU to pause the dispatch of new grids and to hold pending and suspended grids until needed. The GMU also has a direct connection to the Kepler SMX units to permit grids that launch additional work on the GPU via Dynamic Parallelism to send the new work back to GMU to be prioritized and dispatched. If the kernel that dispatched the additional workload pauses, the GMU will hold it inactive until the dependent work has completed.  

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kepler4.png)

Another important feature is Hyper-Q:

In Fermi, though it supports 16-way concurrency kernel launches from separate streams, but ultimately the streams were all multiplexed into the same hardware work queue. This require dependent kernels within one stream to complete before additional kernels in a separate stream could be executed. With Hyper-Q, it increases the total number of connections between the host and the GigaThread work distributor by allowing 32 simultaneous, hardware-managed connections. Hyper-Q is a solution that allows connections from multiple CUDA streams, from multiple Message Passing Interface processes. (Single MPI process may have insufficient work to fully occupy the GPU. Now multiple MPI can share a GPU).

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kepler5.png)

Anonother important feature is GPUDirect:

It allows direct access to GPU memory by third-party devices such as IB adapters, NICs and SSDs without the need for CPU-side data buffering, eliminating CPU bandwidth and latency bottlenecks.

Other features in Kepler:
1. Shuffle instruction (too lazy to explain here)
2. Faster atomic operations
3. More texture units and use textures dynamically
4. More L2 cache and twice the bandwith than Fermi 

### Pascal

Important feature: 
1. Using NVLink connecting GPU-GPU and CPU-GPU: Instead of using PCIe, it use NVLink to provide 5 times bandwidth
2. Using HBM2 memory which provide higher bandwidth for data movement
3. Unified Memory and Allow page fault: With page fault, GPU can over-subscribe the memory beyond the GPU physical device memory. Like virtual memory which give programmer a large memory space larger than the actual memory size, the virtual memory actually comes from disk swapping back and forth with memory. The unified memory unified CPU and GPU memory and allow transparent swap between CPU memory and GPU memory. The benefit of this: 1) Simpler programming and memory model. Then we actually no longer need to write explicit data movement and the data coherency is mantained and kept by unified memory.  2) Complex data structure and C++ classes are much easier to use on GPU. Any hierarchical or nested dat structure can automatically be accessed. 3) Page fault with large page size memory swapped will use the HBM2 large bandwidth well and offer the performance from locality on the GPU. Locality can be got even for programs with sparse data access, where the pages accessd by the CPU or GPU cannot be known ahead of time, and where the CPU and GPU access parts of the same array allocations simultaneously.
4. Compute Preemption: it adds interrupter then allows compute tasks running on the GPU to be interrupted at instruction-level granularity, and their context swapped to GPU DRAM. This permits other applications to be run at the same time smoothly. Like how the CPU run multiple application at the same time. Before Pascal, Kepler and Fermi also allow preemption, but it is at the level of block of threads in a compute kernel. This block-level preemption required that all threads of a thread block complete before the hardware can context switch to a different context.

Other features:
1. Add more double-precision unites and add 16-bit floating point operation for deep learning application.
2. Less CUDA cores per SM but same number of registers and same level of threads and wraps supported per SM. More SM on chip.
3. Better atomics, GPUDirect

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/pascal1.png)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/pascal2.png)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/pascal3.png)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/pascal4.png)

### Volta

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_chip.png)
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_cap.png)

1. Two mode: Maximum Performance mode; Maximum Efficiency Mode;

2. New SM: GP100 SM is partitioned into two processing blocks, each with 32 FP32 Cores, 16 FP64 Cores, an instruction buffer, one warp scheduler, two dispatch units, and a 128 KB Register File. The GV100 SM is partitioned into four processing blocks, each with 16 FP32 Cores, 8 FP64 Cores, 16 INT32 Cores, two of the new mixed-precision Tensor Cores for deep learning matrix arithmetic, a new L0 instruction cache, one warp scheduler, one dispatch unit, and a 64 KB Register File. Note that the new L0 instruction cache is now used in each partition to provide higher efficiency than the instruction buffers used in prior NVIDIA GPUs. While a GV100 SM has the same number of registers as a Pascal GP100 SM, the entire GV100 GPU has far more SMs, and thus many more registers overall. In aggregate, GV100 supports more threads, warps, and thread blocks in flight compared to prior GPU generations.
 
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_sm.png)

3. Tensor Cores

4. Enhanced L1 Data Cache and Shared Memory
Combining data cache and shared memory functionality into a single memory block provides the best overall performance for both types of memory accesses. The combined capacity is 128 KB/SM, more than seven times larger than the GP100 data cache, and all of it is usable as a cache by programs that do not use shared memory. Texture units also use the cache.

Previously, shared memory provides high bandwidth, low latency, and consistent performance (no cache misses), but the CUDA programmer needs to explicitly manage this memory. There is an experiment showing 30% performence loss without using shared memory (just device memory arrays so that accesses would go through L1 cache) in Pascal. Now the cache in Volta can also benefit from shared memory performance. Repeat the same experiment again on Volta, now there is only 7% performance loss without using shared memory. This also means the requirments on programmer to write a high performance code decreases.

5. Simultaneous Execution of FP32 and INT32 Operations

Unlike Pascal GPUs, which could not execute FP32 and INT32 instructions simultaneously, the Volta GV100 SM includes separate FP32 and INT32 cores, allowing simultaneous execution of FP32 and INT32 operations at full throughput, while also increasing instruction issue throughput.

6. NVLINK: higher bandwidth, more links, more features 

Except more links and faster links, The second generation of NVLink allows direct load/store/atomic access from the CPU to each GPU’s HBM2 memory. Coupled with a new CPU mastering capability, NVLink supports coherency operations allowing data reads from graphics memory to be stored in the CPU’s cache hierarchy. The lower latency of access from the CPU’s cache is key for CPU performance. While P100 supported peer GPU atomics, sending GPU atomics across NVLink and completed at the target CPU was not supported. NVLink adds support for atomics initiated by either the GPU or the CPU. Support for Address Translation Services (ATS) has been added allowing the GPU to access the CPU’s page tables directly. A low-power mode of operation for the link has been added allowing for significant power savings when the link is not being heavily used 

7. Faster HBM2 and ECC Memory Resiliency

8. Copy engine enhancemants

The new Volta GV100 GPU copy engines can generate page faults for addresses that are not mapped into the page tables. The memory subsystem can then service the page faults, mapping the addresses into the page table, after which the copy engine can perform the transfer. This is an important enhancement, especially in large multi-GPU / multi-CPU systems, because pinning memory for multiple copy engine operations between multiple processors can substantially reduce available memory. With hardware page faulting, addresses can be passed to the copy engines without worrying if they are resident, and the copy process just works. This feature may be used in ATS systems today

9. Independent thread scheduling

Pascal and earlier NVIDIA GPUs execute groups of 32 threads (known as warps) in SIMT (Single Instruction, Multiple Thread) fashion. The Pascal warp uses a single program counter shared amongst all 32 threads, combined with an active mask that specifies which threads of the warp are active at any given time. This means that divergent execution paths leave some threads inactive, serializing execution for different portions of the warp.
This loss of concurrency means that threads from the same warp in divergent regions or different states of execution cannot signal each other or exchange data. This presents an inconsistency in which threads from different warps continue to run concurrently, but diverged threads from the same warp run sequentially until they reconverge. This means, for example, that algorithms requiring fine-grained sharing of data guarded by locks or mutexes can easily lead to deadlock, depending on which warp the contending threads come from. Therefore, on Pascal and earlier GPUs, programmers need to avoid fine-grained synchronization or rely on lock-free or warp-aware algorithms.

Volta transforms this picture by enabling equal concurrency between all threads, regardless of warp. It does this by maintaining execution state per thread, including a program counter and call stack, as shown below:
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_indsch.png)

Volta’s independent thread scheduling allows the GPU to yield execution of any thread, either to make better use of execution resources or to allow one thread to wait for data to be produced by another. To maximize parallel efficiency, Volta includes a schedule optimizer which determines how to group active threads from the same warp together into SIMT units. This retains the high throughput of SIMT execution as in prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at sub-warp granularity, while the convergence optimizer in Volta will still group together threads which are executing the same code and run them in parallel for maximum efficiency

So previous architecture:
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_indsch_e1.png)

Volta:
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_indsch_e2.png)

It is interesting to note that Figure 22 does not show execution of statement Z by all threads in the warp at the same time. This is because the scheduler must conservatively assume that Z may produce data required by other divergent branches of execution, in which case it would be unsafe to automatically enforce reconvergence. In the common case where A, B, X, and Y do not consist of synchronizing operations, the scheduler can identify that it is safe for the warp to naturally reconverge on Z, as in prior architectures.

Programs can call the new CUDA 9 warp synchronization function \_\_syncwarp() to force reconvergence, as shown below. In this case, the divergent portions of the warp might not execute Z together, but all execution pathways from threads within a warp will complete before any thread reaches the statement after the \_\_syncwarp(). Similarly, placing the call to \_\_syncwarp() before the execution of Z would force reconvergence before executing Z, potentially enabling greater SIMT efficiency if the developer knows that this is safe for their application.

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_indsch_e3.png)

Starvation-free algorithms

Starvation-free algorithms are a key pattern enabled by independent thread scheduling. These are concurrent computing algorithms that are guaranteed to execute correctly so long as the system ensures that all threads have adequate access to a contended resource. For example, a mutex (or lock) may be used in a starvation-free algorithm if a thread attempting to acquire the mutex is guaranteed eventually to succeed. In a system that does not support starvation- freedom, one or more threads may repeatedly acquire and release a mutex while starving another thread from ever successfully acquiring the mutex.

Consider a simplified example that Volta’s independent thread scheduling enables: inserting nodes into a doubly linked list in a multithreaded application.






