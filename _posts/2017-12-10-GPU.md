
layout: post
title: "Notes on GPU"
data: 2017-12-10
tags: [reading notes, GPU]
comments: true
share: false
---

### Brief History of GPU Architecture before Fermi

The first GPU invented by NVIDIA in 1999. From 1999 to 2017, GPU became more general from more specific, become more specific from more general back and forth. From 2003, there are many cool kids hack into the GPU, making it little bit more programmable. Later, by using high-level shading languages such as DirectX, OpenGL, they exploited the GPU for non-graphical application. However, it was fairly ackward since everything has be expressed in terms of vertex coordinates, textures and shader programs. Everything is constrained. NVIDIA is a smart company, they made the first programmed GPU with CUDA enabled: G80. Then GPU computing become easier and signified broader application support. In 2008, NVIDIA introduced the second generation unified architecture GT200: increasing the number of streaming processor cores from 128 to 240. Each processor register file was doubled in size, allowing a greater number of theads to execute on-chip at any given time. Hardward memory access coalescing was added to improve memory acess efficiency and double precision floating point support was added for HPC applications.

### Fermi

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi1.png)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi2.png)

When doing CUDA programming, programmer organizes theads in thread blocks and grid of thread blocks. The GPU instantiates a kernel program on a grid of parallel thread blocks. Each thread within a thread block executes an instance of the kernel, and has a thread ID within its thread block, program counter, registers, per-thread private memory, inputs, and output results. 
A thread block is a set of concurrently executing threads that can cooperate among themselves through barrier synchronization and shared memory. A thread block has a block ID within its grid.

A grid is an array of thread blocks that execute the same kernel, read inputs from global memory, write results to global memory, and synchronize between dependent kernel calls. In the CUDA parallel programming model, each thread has a per-thread private memory space used for register spills, function calls, and C automatic array variables. Each thread block has a per-Block shared memory space used for inter-thread communication, data sharing, and result sharing in parallel algorithms. Grids of thread blocks share results in Global Memory space after kernel-wide global synchronization. 

CUDA's programming hierarchy of thread maps to the hierarchy of processors on GPU; a GPU executes one or more kernel grids; a streaming multiprocessor (SM) executes one or more thread blocks; and CUDA cores and other execution units in the SM execute threads 

So to understand how it works, let's assume we have two application A and B. Application A is composed of two kernels: Kernel1 and Kernel2. Application B is composed of three kernels: Kernel3, Kernel4 and Kernel5. Assume Kernel1 and Kernel2 can use up all the threads the GPU can provide but Kernel1 and Kernel2 are independent. Kernel3, Kernel4 and Kernel5 can only use 1/3 of the computing resource of the GPU and they are independent.

What happens follows:
1. The GigaThread distributor does context switching between application A and B like a CPU doing hyperthreading, where each program receives a time slice of the processor's resources. Say at this time slice, application A is running. Then GigaThread schedules thread blocks to SMs, because Kernel1 and Kernel2 are all mapped to a grid of threads using up all the computing resource, so without specifying multi-stream, only one kernel runs on the SMs. However if we specify 2 mutli-stream, Kernel1 and Kernel2 get scheduled simultaneously and each gets some number of SMs to run on. (However, I am not sure if Fermi allow multi-stream). If say, at this time slice, application B is runnign. Then GigaThread schedulers each kernel with 5 SMs, 5 SMs and 6 SMs and they can run concurrently. However in Fermi, kernels of different application context have to run sequentially with fast contect switching.

⋅⋅⋅So different kernels of same program can run concurrently on different SMs 

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi3.png)

2. 
