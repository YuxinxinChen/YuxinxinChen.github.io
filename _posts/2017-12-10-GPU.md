
layout: post
title: "Notes on GPU"
data: 2017-12-10
tags: [reading notes, GPU]
comments: true
share: false
---

### Brief History of GPU Architecture before Fermi

The first GPU invented by NVIDIA in 1999. From 1999 to 2017, GPU became more general from more specific, become more specific from more general back and forth. From 2003, there are many cool kids hack into the GPU, making it little bit more programmable. Later, by using high-level shading languages such as DirectX, OpenGL, they exploited the GPU for non-graphical application. However, it was fairly ackward since everything has be expressed in terms of vertex coordinates, textures and shader programs. Everything is constrained. NVIDIA is a smart company, they made the first programmed GPU with CUDA enabled: G80. Then GPU computing become easier and signified broader application support. In 2008, NVIDIA introduced the second generation unified architecture GT200: increasing the number of streaming processor cores from 128 to 240. Each processor register file was doubled in size, allowing a greater number of theads to execute on-chip at any given time. Hardward memory access coalescing was added to improve memory acess efficiency and double precision floating point support was added for HPC applications.

### Fermi

!(https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi1.png)

When doing CUDA programming, programmer organizes theads in thread blocks and grid of thread blocks. The GPU instantiates a kernel program on a grid of parallel thread blocks. Each thread within a thread block executes an instance of the kernel, and has a thread ID within its thread block, program counter, registers, per-thread private memory, inputs, and output results. 
A thread block is a set of concurrently executing threads that can cooperate among themselves through barrier synchronization and shared memory. A thread block has a block ID within its grid.

A grid is an array of thread blocks that execute the same kernel, read inputs from global memory, write results to global memory, and synchronize between dependent kernel calls. In the CUDA parallel programming model, each thread has a per-thread private memory space used for register spills, function calls, and C automatic array variables. Each thread block has a per-Block shared memory space used for inter-thread communication, data sharing, and result sharing in parallel algorithms. Grids of thread blocks share results in Global Memory space after kernel-wide global synchronization. 

CUDA's programming hierarchy of thread maps to the hierarchy of processors on GPU; a GPU executes one or more kernel grids; a streaming multiprocessor (SM) executes one or more thread blocks; and CUDA cores and other execution units in the SM execute threads 

> A kernel is mapped to a grid of threads
> 
