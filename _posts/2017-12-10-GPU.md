
layout: post
title: "Notes on GPU"
data: 2017-12-10
tags: [reading notes, GPU]
comments: true
share: false
---

### Brief History of GPU Architecture before Fermi

The first GPU invented by NVIDIA in 1999. From 1999 to 2017, GPU became more general from more specific, become more specific from more general back and forth. From 2003, there are many cool kids hack into the GPU, making it little bit more programmable. Later, by using high-level shading languages such as DirectX, OpenGL, they exploited the GPU for non-graphical application. However, it was fairly ackward since everything has be expressed in terms of vertex coordinates, textures and shader programs. Everything is constrained. NVIDIA is a smart company, they made the first programmed GPU with CUDA enabled: G80. Then GPU computing become easier and signified broader application support. In 2008, NVIDIA introduced the second generation unified architecture GT200: increasing the number of streaming processor cores from 128 to 240. Each processor register file was doubled in size, allowing a greater number of theads to execute on-chip at any given time. Hardward memory access coalescing was added to improve memory acess efficiency and double precision floating point support was added for HPC applications.

### Fermi

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi1.png)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi2.png)

When doing CUDA programming, programmer organizes theads in thread blocks and grid of thread blocks. The GPU instantiates a kernel program on a grid of parallel thread blocks. Each thread within a thread block executes an instance of the kernel, and has a thread ID within its thread block, program counter, registers, per-thread private memory, inputs, and output results. 
A thread block is a set of concurrently executing threads that can cooperate among themselves through barrier synchronization and shared memory. A thread block has a block ID within its grid.

A grid is an array of thread blocks that execute the same kernel, read inputs from global memory, write results to global memory, and synchronize between dependent kernel calls. In the CUDA parallel programming model, each thread has a per-thread private memory space used for register spills, function calls, and C automatic array variables. Each thread block has a per-Block shared memory space used for inter-thread communication, data sharing, and result sharing in parallel algorithms. Grids of thread blocks share results in Global Memory space after kernel-wide global synchronization. 

CUDA's programming hierarchy of thread maps to the hierarchy of processors on GPU; a GPU executes one or more kernel grids; a streaming multiprocessor (SM) executes one or more thread blocks; and CUDA cores and other execution units in the SM execute threads 

So to understand how it works, let's assume we have two application A and B. Application A is composed of two kernels: Kernel1 and Kernel2. Application B is composed of three kernels: Kernel3, Kernel4 and Kernel5. Assume Kernel1 and Kernel2 can use up all the threads the GPU can provide but Kernel1 and Kernel2 are independent. Kernel3, Kernel4 and Kernel5 can only use 1/3 of the computing resource of the GPU and they are independent.

What happening follows:
1. The GigaThread distributor does context switching between application A and B like a CPU doing hyperthreading, where each program receives a time slice of the processor's resources. Say at this time slice, application A is running. Then GigaThread schedules thread blocks to SMs, because Kernel1 and Kernel2 are all mapped to a grid of threads using up all the computing resource, so without specifying multi-stream, only one kernel runs on the SMs. However if we specify 2 mutli-stream, Kernel1 and Kernel2 get scheduled simultaneously and each gets some number of SMs to run on but will require more time to finish each kernels. If say, at this time slice, application B is runnign. Then GigaThread might schedulers each kernel with 5 SMs, 5 SMs and 6 SMs and they can run concurrently. However in Fermi, kernels of different application context have to run sequentially with fast contect switching.

+ So different kernels of same program can run concurrently on different SMs. For Fermi, it can run concurrent kernels up to 16 (Fermi has 16 SMs)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/fermi3.png)

2. Each SM gets the thread block from GigaThread. It schedules threads in warps. Each SM features two warp schedulers and two instruction dispatch units, allowing two warps to be issued and executed concurrently. Fermi's warp schedular doesn't need to check for dependencies from within the instruction stream as the figure above shows. So if you programlly define the thread block size very large, then it will scheduler several wraps to finish the work. Why using the dual-issue model? In the above Fermi SM figure, it only has 16 CUDA cores, but in GF100, the number of CUDA cores increases to 32, in GF104, the number of CUDA cores increase to 48, while the number of SFUs increases to 8. For Fermi, a warp is executed over 2 (or more) clocks of the CUDA cores – 16 threads are processed and then the other 16 threads in that warp are processed. For full SM utilization, all threads must be running the same instruction at the same time. For these reasons a SM is internally divided up in to a number of execution units that a single dispatch unit can dispatch work to:

> 16 CUDA cores (#1)

> 16 CUDA cores (#2)

> 16 Load/Store Units

> 16 Interpolation SFUs

> 4 Special Function SFUs

> 4 Texture Units

With 2 warp scheduler/dispatch unit pairs in each SM, GF100 can utilize at most 2 of 6 execution units at any given time. It’s also because of the SM being divided up like this that it was possible for NVIDIA to add to it. GF104 in comparison has the following:

> 16 CUDA cores (#1)

> 16 CUDA cores (#2)

> 16 CUDA cores (#3)

> 16 Load/Store Units

> 16 Interpolation SFUs

> 8 Special Function SFUs

> 8 Texture Units

This gives GF104 a total of 7 execution units, the core of which are the 3 blocks of 16 CUDA cores.
With 2 warp schedulers, GF100 could put all 32 CUDA cores to use if it had 2 warps where both required the use of CUDA cores. With GF104 this gets more complex since there are now 3 blocks of CUDA cores but still only 2 warp schedulers. So how does NVIDIA feed 3 blocks of CUDA cores with only 2 warp schedulers? They go superscalar.

In a nutshell, superscalar execution is a method of extracting Instruction Level Parallelism from a thread. If the next instruction in a thread is not dependent on the previous instruction, it can be issued to an execution unit for completion at the same time as the instruction preceding it. There are several ways to extract ILP from a workload, with superscalar operation being something that modern CPUs have used as far back as the original Pentium to improve performance. For NVIDIA however this is new – they were previously unable to use ILP and instead focused on Thread Level Parallelism (TLP) to ensure that there were enough warps to keep a GPU occupied.

+ Each SM at each time slice is assigned one or more thread blocks from one or more kernels
+ SM scheduler selects two warps and issues one instruction from each warp to a group of 16 cores, 16 load/store units or 4 SFUs. The two warps can be from different thread blocks then might be from different kernels?? There is no requirement for the warp schedulers to select two warps from the same thread block.
+ When issuing two warps, the scheduler doesn't check the instruction dependency between those two warps.
+ Within a warp, it uses superscalar to fully utlize the GPU. 

Other features Fermi has:
1. Improve Double Precision Performance
2. Add ECC support
3. Add Cache hierarchy: configurable and more L1/shared memory + L2 cache
4. Faster Context Switching and Atomic Operations 
5. More CUDA cores per SM and more SMs
6. Unified Address Space with C++ support
7. Improved performance through prediction
8. Dual overlapped memory transfer engines
9. Out of order thread block execution

Usefull links: NVIDIA Fermi Compute Architecture Whitepaper <http://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf> and GF104 Reviews <https://www.anandtech.com/show/3809/nvidias-geforce-gtx-460-the-200-king/2> and Stackoverflow Question <https://stackoverflow.com/questions/10460742/how-do-cuda-blocks-warps-threads-map-onto-cuda-cores>

### Kepler 

Kepler is more targeting on HPC market while Maxwell is more on graphics market. First a Kepler GK110 chip block diagram:

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kepler1.png)

Weird layout with 15 SMX (not SM), still 6 memory controller, L2 Cache shared by all SMX. But new SMX is much much powerfull. Let's see an overall infor about kepler below: each SMX, it can support more threads (add CUDA cores) and more thread blocks (more schedulers and dispatch units) and more registers (each thread need register to put temp data thendo fast context switch. Then more registers can prevent spill and support more threads). 


![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kepler2.png)

A detailed look of SMX:

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kepler3.png)

The first look is much much more CUDA cores and DP units and load/store units and special function unites on one SMX and much more registers and texture units. Also there are 4 warp schedulers and 8 dispatch units. It add 48KB read-only data cache.

* 192 CUDA cores, 64 double-precision units, 32 SFU and 32 L/S units.
* 255 register per thread.
* Add read-only data cache. 
* 4 warp schedulers, 8 dispatch units, allowing 4 warps to run concurrently.

So after adding so many cores, to keep energy efficiency, kepler use many less powerful cores (lower clock rate but fully pipelined floating-point and integer arithmetic units).
Because it now has 4 warp schedulers and 8 dispatch units, it allows four warps to be issued and executed concurrently.
For warp scheduler also be optimized in other aspects:
1. Register scoreboarding for long latency operations (texture and load)
2. Inter-warp scheduling decisions (pick the best warp to go next among eligible candidates)
3. Thread block level scheduling 
4. Instead having a complex hardware stage to prevent data hazards(A multi-port register scoreboard keeps track of any registers that are not ready with valid data, and a dependency checker blocl analyzes register usage across a multitude of fully decoded warp instructions against the scoreboard, to determine which are eligible to issue), Kepler determine the dependency at compile time, thus replacing several power-hungry blocks.

I think an important feature Kepler has is DYNAMIC PARALLELISM:
So basically, it add grid management unit (GMU), which collects the task generately dynamically on GPU and schedule them appropriatly. In kepler, a grid can be launched from the CPU just as was the case with Fermi: GigaThread work distributor distribute the thread blocks to SMXs, but CUDA-created grids will be managed and prioritized by grid management unit then passed into Gigathread work distributor which send to SMX for execution. 
The GigaThread work distributor in Kepler holds grids that are ready to dispatch, and it is able to dispatch 32 active grids, which is double the capacity of the Fermi GigaThread work distributor. The Kepler GigaThread work distributor communicates with the GMU via a bidirectional link that allows the GMU to pause the dispatch of new grids and to hold pending and suspended grids until needed. The GMU also has a direct connection to the Kepler SMX units to permit grids that launch additional work on the GPU via Dynamic Parallelism to send the new work back to GMU to be prioritized and dispatched. If the kernel that dispatched the additional workload pauses, the GMU will hold it inactive until the dependent work has completed.  

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kepler4.png)

Another important feature is Hyper-Q:

In Fermi, though it supports 16-way concurrency kernel launches from separate streams, but ultimately the streams were all multiplexed into the same hardware work queue. This require dependent kernels within one stream to complete before additional kernels in a separate stream could be executed. With Hyper-Q, it increases the total number of connections between the host and the GigaThread work distributor by allowing 32 simultaneous, hardware-managed connections. Hyper-Q is a solution that allows connections from multiple CUDA streams, from multiple Message Passing Interface processes. (Single MPI process may have insufficient work to fully occupy the GPU. Now multiple MPI can share a GPU).

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/kepler5.png)

Anonother important feature is GPUDirect:

It allows direct access to GPU memory by third-party devices such as IB adapters, NICs and SSDs without the need for CPU-side data buffering, eliminating CPU bandwidth and latency bottlenecks.

Other features in Kepler:
1. Shuffle instruction (too lazy to explain here)
2. Faster atomic operations
3. More texture units and use textures dynamically
4. More L2 cache and twice the bandwith than Fermi 

### Pascal

Important feature: 
1. Using NVLink connecting GPU-GPU and CPU-GPU: Instead of using PCIe, it use NVLink to provide 5 times bandwidth
2. Using HBM2 memory which provide higher bandwidth for data movement
3. Unified Memory and Allow page fault: With page fault, GPU can over-subscribe the memory beyond the GPU physical device memory. Like virtual memory which give programmer a large memory space larger than the actual memory size, the virtual memory actually comes from disk swapping back and forth with memory. The unified memory unified CPU and GPU memory and allow transparent swap between CPU memory and GPU memory. The benefit of this: 1) Simpler programming and memory model. Then we actually no longer need to write explicit data movement and the data coherency is mantained and kept by unified memory.  2) Complex data structure and C++ classes are much easier to use on GPU. Any hierarchical or nested dat structure can automatically be accessed. 3) Page fault with large page size memory swapped will use the HBM2 large bandwidth well and offer the performance from locality on the GPU. Locality can be got even for programs with sparse data access, where the pages accessd by the CPU or GPU cannot be known ahead of time, and where the CPU and GPU access parts of the same array allocations simultaneously.
4. Compute Preemption: it adds interrupter then allows compute tasks running on the GPU to be interrupted at instruction-level granularity, and their context swapped to GPU DRAM. This permits other applications to be run at the same time smoothly. Like how the CPU run multiple application at the same time. Before Pascal, Kepler and Fermi also allow preemption, but it is at the level of block of threads in a compute kernel. This block-level preemption required that all threads of a thread block complete before the hardware can context switch to a different context.

Other features:
1. Add more double-precision unites and add 16-bit floating point operation for deep learning application.
2. Less CUDA cores per SM but same number of registers and same level of threads and wraps supported per SM. More SM on chip.
3. Better atomics, GPUDirect

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/pascal1.png)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/pascal2.png)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/pascal3.png)

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/pascal4.png)

### Volta

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_chip.png)
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_cap.png)

1. Two mode: Maximum Performance mode; Maximum Efficiency Mode;

2. New SM: GP100 SM is partitioned into two processing blocks, each with 32 FP32 Cores, 16 FP64 Cores, an instruction buffer, one warp scheduler, two dispatch units, and a 128 KB Register File. The GV100 SM is partitioned into four processing blocks, each with 16 FP32 Cores, 8 FP64 Cores, 16 INT32 Cores, two of the new mixed-precision Tensor Cores for deep learning matrix arithmetic, a new L0 instruction cache, one warp scheduler, one dispatch unit, and a 64 KB Register File. Note that the new L0 instruction cache is now used in each partition to provide higher efficiency than the instruction buffers used in prior NVIDIA GPUs. While a GV100 SM has the same number of registers as a Pascal GP100 SM, the entire GV100 GPU has far more SMs, and thus many more registers overall. In aggregate, GV100 supports more threads, warps, and thread blocks in flight compared to prior GPU generations.
 
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_sm.png)

3. Tensor Cores

4. Enhanced L1 Data Cache and Shared Memory
Combining data cache and shared memory functionality into a single memory block provides the best overall performance for both types of memory accesses. The combined capacity is 128 KB/SM, more than seven times larger than the GP100 data cache, and all of it is usable as a cache by programs that do not use shared memory. Texture units also use the cache.

Previously, shared memory provides high bandwidth, low latency, and consistent performance (no cache misses), but the CUDA programmer needs to explicitly manage this memory. There is an experiment showing 30% performence loss without using shared memory (just device memory arrays so that accesses would go through L1 cache) in Pascal. Now the cache in Volta can also benefit from shared memory performance. Repeat the same experiment again on Volta, now there is only 7% performance loss without using shared memory. This also means the requirments on programmer to write a high performance code decreases.

5. Simultaneous Execution of FP32 and INT32 Operations

Unlike Pascal GPUs, which could not execute FP32 and INT32 instructions simultaneously, the Volta GV100 SM includes separate FP32 and INT32 cores, allowing simultaneous execution of FP32 and INT32 operations at full throughput, while also increasing instruction issue throughput.

6. NVLINK: higher bandwidth, more links, more features 

Except more links and faster links, The second generation of NVLink allows direct load/store/atomic access from the CPU to each GPU’s HBM2 memory. Coupled with a new CPU mastering capability, NVLink supports coherency operations allowing data reads from graphics memory to be stored in the CPU’s cache hierarchy. The lower latency of access from the CPU’s cache is key for CPU performance. While P100 supported peer GPU atomics, sending GPU atomics across NVLink and completed at the target CPU was not supported. NVLink adds support for atomics initiated by either the GPU or the CPU. Support for Address Translation Services (ATS) has been added allowing the GPU to access the CPU’s page tables directly. A low-power mode of operation for the link has been added allowing for significant power savings when the link is not being heavily used 

7. Faster HBM2 and ECC Memory Resiliency

8. Copy engine enhancemants

The new Volta GV100 GPU copy engines can generate page faults for addresses that are not mapped into the page tables. The memory subsystem can then service the page faults, mapping the addresses into the page table, after which the copy engine can perform the transfer. This is an important enhancement, especially in large multi-GPU / multi-CPU systems, because pinning memory for multiple copy engine operations between multiple processors can substantially reduce available memory. With hardware page faulting, addresses can be passed to the copy engines without worrying if they are resident, and the copy process just works. This feature may be used in ATS systems today

9. Independent thread scheduling

Pascal and earlier NVIDIA GPUs execute groups of 32 threads (known as warps) in SIMT (Single Instruction, Multiple Thread) fashion. The Pascal warp uses a single program counter shared amongst all 32 threads, combined with an active mask that specifies which threads of the warp are active at any given time. This means that divergent execution paths leave some threads inactive, serializing execution for different portions of the warp.
This loss of concurrency means that threads from the same warp in divergent regions or different states of execution cannot signal each other or exchange data. This presents an inconsistency in which threads from different warps continue to run concurrently, but diverged threads from the same warp run sequentially until they reconverge. This means, for example, that algorithms requiring fine-grained sharing of data guarded by locks or mutexes can easily lead to deadlock, depending on which warp the contending threads come from. Therefore, on Pascal and earlier GPUs, programmers need to avoid fine-grained synchronization or rely on lock-free or warp-aware algorithms.

Volta transforms this picture by enabling equal concurrency between all threads, regardless of warp. It does this by maintaining execution state per thread, including a program counter and call stack, as shown below:
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_indsch.png)

Volta’s independent thread scheduling allows the GPU to yield execution of any thread, either to make better use of execution resources or to allow one thread to wait for data to be produced by another. To maximize parallel efficiency, Volta includes a schedule optimizer which determines how to group active threads from the same warp together into SIMT units. This retains the high throughput of SIMT execution as in prior NVIDIA GPUs, but with much more flexibility: threads can now diverge and reconverge at sub-warp granularity, while the convergence optimizer in Volta will still group together threads which are executing the same code and run them in parallel for maximum efficiency

So previous architecture:
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_indsch_e1.png)

Volta:
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_indsch_e2.png)

It is interesting to note that Figure 22 does not show execution of statement Z by all threads in the warp at the same time. This is because the scheduler must conservatively assume that Z may produce data required by other divergent branches of execution, in which case it would be unsafe to automatically enforce reconvergence. In the common case where A, B, X, and Y do not consist of synchronizing operations, the scheduler can identify that it is safe for the warp to naturally reconverge on Z, as in prior architectures.

Programs can call the new CUDA 9 warp synchronization function \_\_syncwarp() to force reconvergence, as shown below. In this case, the divergent portions of the warp might not execute Z together, but all execution pathways from threads within a warp will complete before any thread reaches the statement after the \_\_syncwarp(). Similarly, placing the call to \_\_syncwarp() before the execution of Z would force reconvergence before executing Z, potentially enabling greater SIMT efficiency if the developer knows that this is safe for their application.

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_indsch_e3.png)

Starvation-free algorithms

Starvation-free algorithms are a key pattern enabled by independent thread scheduling. These are concurrent computing algorithms that are guaranteed to execute correctly so long as the system ensures that all threads have adequate access to a contended resource. For example, a mutex (or lock) may be used in a starvation-free algorithm if a thread attempting to acquire the mutex is guaranteed eventually to succeed. In a system that does not support starvation- freedom, one or more threads may repeatedly acquire and release a mutex while starving another thread from ever successfully acquiring the mutex.

Consider a simplified example that Volta’s independent thread scheduling enables: inserting nodes into a doubly linked list in a multithreaded application.
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_insert.png)

In this example, each element of a doubly linked list has at least three components: a next pointer, a previous pointer, and a lock providing the owner exclusive access to update the node. Figure 24 shows the insertion of node B after node A with updates to the next and previous pointers of A and C.
Independent thread scheduling in Volta ensures that even if a thread T0 currently holds the lock for node A, another thread T1 in the same warp can successfully wait for the lock to become available without impeding the progress of thread T0. Note, however, that because active threads in a warp execute together, threads spinning on a lock may degrade the performance of the thread holding the lock.
It is also important to note that the use of a per-node lock in the above example is critical for performance on the GPU. Traditional doubly-linked list implementations may use a coarse- grained lock that provides exclusive access to the entire structure, rather than separately protecting individual nodes. This approach typically leads to poor performance in applications with many threads—Volta may have up to 163,840 concurrent threads—caused by extremely high contention for the lock. By using a fine-grained lock on each node, the average per-node contention in large lists will usually be low except under certain pathological node insertion patterns.
This doubly-linked list with fine-grained locks is a simple example, but it demonstrates how independent thread scheduling gives developers the capability to implement familiar algorithms and data structures on the GPU in a natural way.

10. Volta multi-process service
Volta Multi-Process Service (MPS) is a new feature of the Volta GV100 architecture enabling improved performance and isolation for multiple compute applications sharing the GPU. Typical execution of multiple applications sharing the GPU is implemented with time-slicing, that is, each application gets exclusive access for a period of time before access is granted to another application. Volta MPS improves aggregate GPU utilization by allowing multiple applications to simultaneously share GPU execution resources when these applications individually under-utilize the GPU execution resources.
Starting with Kepler GK110 GPUs, NVIDIA introduced a software-based Multi-Process Service (MPS) and MPS Server that allowed multiple different CPU processes (application contexts) to be combined into a single application context and run on the GPU, attaining higher GPU resource utilization.
Volta MPS provides hardware acceleration of critical components of the MPS server for improved performance and isolation, while increasing the maximum number of MPS clients from 16 on Pascal up to 48 on Volta (see Figure 25). Volta Multi-Process service is designed for sharing the GPU amongst applications from a single user and is not for multi-user or multi-tenant use cases.
For Pascal, CUDA Multi-Process Service is a CPU process which acts on behalf of GPU applications that have requested to simultaneously share execution resources with other GPU applications. This process acts as the intermediary to submit work to the work queues inside the GPU for concurrent kernel execution.
The Volta Multi-Process Service provides hardware acceleration of CUDA MPS which enables MPS clients to submit work directly to the work queues within the GPU. This acceleration significantly decreases submission latency and increases aggregate throughput. For Volta, the CPU MPS control process remains for configuration and opt-in to the MPS.

11. Unified memory and address translation service 
A limited form of Unified Memory was introduced with CUDA 6 in our Kepler and Maxwell GPUs, and it was improved with hardware page faulting and a larger address space in the Pascal GP100 GPU. Unified memory allows a single unified virtual address space for CPU and GPU memory, greatly simplifying GPU programming and porting of applications to GPUs. Programmers no longer need to worry about managing data sharing between GPU and CPU virtual memory systems. Unified Memory in Pascal GP100 provided transparent migration of data between the full virtual address spaces of both the GPU and CPU. (For a detailed explanation of Pascal Unified Memory technology, please see Pascal White Paper<https://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper.pdf>)

Although Unified Memory in Pascal GP100 improved CUDA programming in many ways, Volta GV100 further improves efficiency and performance of Unified Memory. A new Access Counter feature keeps track of the frequency of access that a GPU makes to memory located on other processors. Access Counters help ensure memory pages are moved to the physical memory of the processor that is accessing the pages most frequently. The Access Counters feature can work in either NVLink- or PCIe-connected GPU-CPU or GPU-GPU architectures, and can work with different types of CPUs including Power 9, x86, and others.

Volta also supports Address Translation Services (ATS) over NVLink. ATS allows the GPU to directly access the CPU’s page tables. A miss in the GPU MMU will result in an Address Translation Request (ATR) to the CPU. The CPU looks in its page tables for the virtual-to-physical mapping for that address and supplies the translation back to the GPU. ATS provides the GPU full access to CPU memory, for example to memory allocated directly with ‘malloc’.

12. Cooperative groups
Cooperative Groups is a new programming model for organizing groups of threads.
Historically, the CUDA programming model has provided a single, simple construct for synchronizing cooperating threads: a barrier across all threads of a thread block, as implemented with the \_\_syncthreads( ) function. However, programmers would often like to define groups of threads at smaller than thread block granularities and synchronize within them to enable greater performance, design flexibility, and software reuse in the form of “collective” group-wide function interfaces.
Cooperative Groups introduces the ability to define groups of threads explicitly at sub-block and multiblock granularities, and to perform collective operations such as synchronization on them. This programming model supports clean composition across software boundaries, so that libraries and utility functions can synchronize safely within their local context without having to make assumptions about convergence. It lets developers optimize for the hardware fast path— for example the GPU warp size—using flexible synchronization in a safe, supportable way that makes programmer intent explicit. Cooperative Groups primitives enable new patterns of cooperative parallelism within CUDA, including producer-consumer parallelism, opportunistic parallelism, and global synchronization across the entire grid.
Cooperative Groups also provides an abstraction by which developers can write flexible, scalable code that will work safely across different GPU architectures, including scaling to future GPU capabilities. Thread groups may range in size from a few threads (smaller than a warp) to a whole thread block, to all thread blocks in a grid launch, to grids spanning multiple GPUs.
While Cooperative Groups works on all GPU architectures, certain functionality is inevitably architecture-dependent as GPU capabilities have evolved. Basic functionality, such as synchronizing groups smaller than a thread block down to warp granularity, is supported on all architectures, while Pascal and Volta GPUs enable new grid-wide and multi-GPU synchronizing groups. In addition, Volta’s independent thread scheduling enables significantly more flexible selection and partitioning of thread groups at arbitrary cross-warp and sub-warp granularities. Volta synchronization is truly per thread: threads in a warp can synchronize from divergent code paths.

The Cooperative Groups programming model consists of the following elements:
  1. New mixed-precision FP16/FP32 Tensor Cores purpose-built for deep learning matrix arithmetic
  2. Data types for representing groups of cooperating threads;
  3. Default groups defined by the CUDA launch API (e.g., thread blocks and grids);
  4. Operations for partitioning existing groups into new groups;
  5. A barrier operation to synchronize all threads within a group;
  6. Operations to inspect the group properties as well as group-specific collectives.

An example:

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volt_copthr_e.png)

Cooperative Groups uses C++ templates to provide types and API overloads to represent groups whose size is statically determined for even greater efficiency. The language-level interface is supported by a set of PTX assembly extensions that provide the substrate for the CUDA C++ implementation. These PTX extensions are also available to any programming system that wants to provide similar functionality. Finally, the race detection tool in cuda-memcheck and the CUDA debugger are compatible with the more flexible synchronization patterns permitted by Cooperative Groups, to make it easier to find subtle parallel synchronization bugs such as Read After Write (RAW) hazards.
Cooperative Groups allows programmers to express synchronization patterns that they were previously unable to express. When the granularity of synchronization corresponds to natural architectural granularities (warps and thread blocks), the overhead of this flexibility is negligible. Libraries of collective primitives written using Cooperative Groups often require less complex code to achieve high performance.
Consider a particle simulation, where we have two main computation phases in each step of the simulation. First, integrate the position and velocity of each particle forward in time. Second, build a regular grid spatial data structure to accelerate finding collisions between particles.

![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_e1.png)
Before Cooperative Groups, implementing such a simulation required multiple kernel launches, because the mapping of threads changes from phase 1 to phase 2. The process of building the regular grid acceleration structure reorders particles in memory, necessitating a new mapping of threads to particles. Such a remapping requires synchronization among threads. The implicit synchronization between back-to-back kernel launches satisfies this requirement, as the following CUDA pseudocode shows.
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_e2.png)
Cooperative Groups provides flexible and scalable thread group types and synchronization primitives enable parallelism remapping in situations like the above example within a single kernel launch. The following CUDA kernel provides a sketch of how the particle system update could be updated in a single kernel. The use of this_grid() defines a thread group comprising all threads of the kernel launch, which is then synchronized between the two phases.
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_e3.png)
This kernel is written in such a way that it is trivial to extend the simulation to multiple GPUs. The Cooperative Groups function this_multi_grid() returns a thread group spanning all threads of a kernel launch across multiple GPUs. Calling sync() on this group synchronizes all threads running the kernel on multiple GPUs. Note that in both cases, the thread_rank() method provides a linear index of the current thread within the thread group, which the kernel uses to iterate over the particles in parallel in case there are more particles than threads.
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/volta_e4.png)
To use groups that span multiple thread blocks or multiple GPUs, applications must use the cudaLaunchCooperativeKernel() or cudaLaunchCooperativeKernelMultiDevice() API, respectively. Synchronization requires that all thread blocks are simultaneously resident, so the application must also ensure that the resource usage (registers and shared memory) of the thread blocks launched does not exceed the total resources of the GPU(s).

