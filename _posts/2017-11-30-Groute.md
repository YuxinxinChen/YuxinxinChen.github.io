
layout: post
title: "Notes on Groute"
data: 2017-11-30
tags: [reading notes, asynchronous, graph]
comments: true
share: false
---

### Overview about Groute Connect Components

Groute uses adaptive CC. On single machine, each thread takes an edge and does an atomic hook (some threads will compete to hook same node, then use atomic to ensure no overwriting happen. So this hook is called atomic hook). After that, we do a compression to flatten the tree into 1-level tree. Then the algorith ends. On multi-GPU, because remote atomic is expensive and sometimes, it is just simply infeasiable. So edges are partitioned and given to each GPU. Each GPU does local atomic hook and local compression regardless if some local connected components are disconnected because that linking edge is on other GPUs. After local hook and compresson are done, the CC information are reduced according to a policy ( one way: O(N), tree: O(logN) N is number of GPUs). The GPU/GPUs who reducing the CC information, because the received CC informaiton is also a 1-level tree, i.e. number of edges = number of nodes, then we can simply rerun the actomic hook and compression on received CC information, information is integrated.

One feature of Groute, which is also heavily used in Groute CC is its data transfer & compute pipeline. So the data are divided into chunks and each chunks are distributed in a round robin way to all the GPUs. Each GPU, after receiving the first chunk of data, it can start to do computation on that chunk of data without having to wait all the assigned data finishing transferring.The data tranfer and data computation can happen concurrently. GPU has two indepentant MMU to do data tranferring without interfering computation parts.

To support pipelining, there are several important data structure needed to understand. See below figure from a soul painter:
First Router is abstraction for communication and links is an abstraction connecting physical devices and routers. Then naive idea of the following structure used in CC is that: 
In input\_router, send\_link connects host and input\-router and is used for sending data, edges\_in[0]...edges\_in[N] connect input\_router and GPUs and is used for receiving data. 
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_input.png)
In reduction\_router, receive\_link connects host and reduction\_router for receiving data, reduction\_in[0]...reduction\_in[N] connects reduction\_router and GPUs for sending data. Here there is no link sending back from reduction\_router to host. So those reduction\_in[0]...reduction\_in[N] will be used to send data around GPUs and reduction\_out[0]...reduction\_out[N] connect reduction\_router for receiving data from other GPUs. So reduction\_router is basically used for communication between GPUs and send the final result to Host(CPU) when done.
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_reduct.png)
The links are attached to its router. Different routers are different system and has its own links like blood vessel sending blood to parts of body.

A detail data structure help understand. The link will construct a PipelinedSender or a PipelinedReceiver internally depending on the direction of the link. In PipelineSender constructs pipeline and m\_promise\_buff and m\_sender internally and m\_sender will stored under corresponding router. In PipelineReceiver, it constructs pipeline and m\_promise\_seg and m\_receiver internally and the m\_receiver will be stored under corresponding router. So all the links are constructed under input\_router and the PipelineSender will push send\_operation into corresponding PipelineReceivr's m\_receiver's m\_send\_queue just by asking the router for the destination address. PipelineReciever will push receive\_operation into its own m\_receiver's m\_receive\_queue. send\_op and receive\_op are paired when they are both got poped out and the corresponding memory copy work will be queue onto stream. When the memory copy work finishes, the call back function will update how much memory copy the send\_op has been done on send\_op so far and set the promise value of receive\_op. Then the computation who is waiting on that future gets unblocked. The red dash line in the below figure means the data structure can be access from router.
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_datastruct.png)
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_sendop.png)
So usually the send\_op is larger than receive\_op, there will be several receive\_op to consume one send\_op.

Mapping to Groute framework. Data are offloaded to GPU chunk by chunk via the sender and receiver. There is a flag: compute\_latency\_ratio, which define how much data are offloaded to GPU in advance before start computation. There is also an option to enable fragmentation transferring. The policy of router will determine how the set up network communicate. For input\_router, it broadcasts data to all GPUs and for reduction\_router, each GPU transfers its data to its level up boss. 

After GPUs have the data, it starts computation and then wait on next chunk's future untill consume all the data assigned. Then pipelined data transfer happens again from GPU to another GPU based on routing policy and the GPU who receives the data starts computation when it gets the first chunk of data. The reduction tree continues as all the information are integrated.

The main code of Groute is below:
```bash
bool RunCCMAsyncAtomic(int ngpus)
{
    cc::Context context(FLAGS_graphfile, FLAGS_ggr, FLAGS_verbose, ngpus); // Create context

    cc::Configuration configuration;  // Create configuration object and pass the configuration
    cc::BuildConfigurationAuto( // Deduce the configuration automatically  
    .....)                       // Configuration Code ignored

    context.DisableFragmentation();  // The fragementation means the size of fragment of data when we transfer data, so even if the data transfer is pipelined, each chunk can be tansferred fragmently
    
    double par_total_ms = 0.0, total_ms = 0.0; 
    for (size_t rep = 0; rep < FLAGS_repetitions; ++rep)
    {
        Stopwatch psw(true);

        groute::Segment<Edge> all_edges = groute::Segment<Edge>(&context.host_edges[0], context.nedges, context.nedges, 0); // Save all edges into Segment object
        cc::EdgePartitioner partitioner(ngpus, context.nvtxs, all_edges, configuration.vertex_partitioning); // Partition the edges into several Segments

        psw.stop(); // Partitioning time
        par_total_ms += psw.ms();

        auto reduction_policy = FLAGS_tree_topology // Define reduction policy
            ? groute::Policy::CreateTreeReductionPolicy(ngpus)
            : groute::Policy::CreateOneWayReductionPolicy(ngpus);

        groute::Router<Edge> input_router(context, (std::shared_ptr<groute::IPolicy>)std::make_shared<cc::EdgeScatterPolicy>(ngpus), 1, ngpus); // Create input router
        groute::Router<component_t> reduction_router(context, reduction_policy, ngpus, ngpus+1); // Create reduction router

        groute::Endpoint host = groute::Endpoint::HostEndpoint(0); // Int ID of Host, usually 0
        groute::Link<Edge> send_link(host, input_router); // Create link from host to input_router
        groute::Link<component_t> receive_link(reduction_router, host, 0, 0); // Create link from reduction_router to host no pipelining here 

        std::vector< std::unique_ptr<cc::Problem> > problems; // Create problem object which specify what operation the algorithm CC has and its corresponding data structure
        std::vector< std::unique_ptr<cc::Solver> > solvers; // Create solver object which construct the algorithm using the block operation in problem object
        std::vector<std::thread> workers(ngpus); // Each worker thread will transfer data and start the computation to each GPU

        dim3 block_dims(MASYNC_BS, 1, 1);

        for (int i = 0; i < ngpus; ++i) // Each GPU has its problem and solver and links
        {
            problems.emplace_back(new cc::Problem(context, partitioner.parents_partitions[i], i, block_dims)); // Each GPU has it is own problem defined by the data assigned
            solvers.emplace_back(new cc::Solver(context, *problems.back())); // Solver per problem

            solvers[i]->edges_in = groute::Link<Edge>(input_router, i, configuration.edges_chunk_size, configuration.input_pipeline_buffers); 
	    ///////////////////////////////////////////////////////////////////////
            // edges_in link per GPU to get data from input_router. The link will//
            // link will construct a pipelinedReceiver which in itsconstruction, //
            // number of input_pipeline_buffers of receive_op are pushed onto its//
            // m_receiver's m_queue_receive and receive_op's future are pushe    //
            // onto m_promise_seg.                                               //
            // input_pipeline_buffers is computed from compute_latency_ratio     //
	    // which determine how much data are offloaded to the GPU in advance //
	    // Note if this ratio=0, all data are transferred on GPUs before     //
            // computation begins                                                //
            ///////////////////////////////////////////////////////////////////////

            solvers[i]->reduction_in = groute::Link<component_t>(reduction_router, i, configuration.parents_chunk_size, configuration.reduction_pipeline_buffers); // reduction_in link to reduction_router to get data from other GPU since every GPU also has a reduction_out link to reduction_router
            solvers[i]->reduction_out = groute::Link<component_t>(i, reduction_router); // reduction_out link to reduction_router to send data to other GPU since every GPU also has a reduction_in link to reduction_router
        }

        IntervalRangeMarker iter_rng(context.nedges, "begin");

        for (auto& edge_partition : partitioner.edge_partitions) // for each partition, clikc send buttom
        {
            send_link.Send(edge_partition, groute::Event()); 
	    ///////////////////////////////////////////////////////////////////////
            // send_link.Send call m_sender.send:                                //
            // 1. Create EventGroupPromises                                      //
            // 2. Create send_op                                                 //
            // 3. queue send_op in that partition target device's m_send_queue   //
            // 4. call partition target device's m_receiver's Assigne()--------> //
	    // pair a receive_op with a send_op and pop the receive_op, reduce   //
	    // the send_op size (pop send_op when all send_op is consumed) then  // 
            // call input_router.queueMemcpyWork(receive_op,send_op)------> when //
            // the copywork finish, a call back of queueMemcpyWork call:         //
            // receive_op.complete()                                             //
            // send_op.reportProgress(size)                                      //
            // Though the mem copy work hasn't got chance, the code continue to  //
            // execute since it is unblocking, somewhere later in the code who   //
            // call get() on receive_op's future may block on that since the copy//
            // work hasn't finished yet                                          // 
	    ///////////////////////////////////////////////////////////////////////
        }
        send_link.Shutdown();

        for (int i = 0; i < ngpus; ++i)
        {
            // Sync the first pipeline copy operations (exclude from timing)
            solvers[i]->edges_in.PipelineSync();
	    // For each GPU, number of input_pipeline_buffer chunks of data finish transfering data to GPU
        }

        groute::internal::Barrier barrier(ngpus + 1); // barrier for accurate timing  

        for (int i = 0; i < ngpus; ++i)
        {
            // Run workers, each one launchs work on each GPU
            std::thread worker(
                [&configuration, &barrier](cc::Solver& solver)
            {
                barrier.Sync();
                barrier.Sync();
                solver.Solve(configuration); // start the CC algorithm, unblocking
            },
                std::ref(*solvers[i]));

            workers[i] = std::move(worker);
        }

        barrier.Sync();     // sync for accurate start timing 
        Stopwatch sw(true); // all threads are running, start timing  
        barrier.Sync();     // and signal

        for (int i = 0; i < ngpus; ++i)
        {
            // Join threads  
            workers[i].join();  //threads will join here when solver.solve finishs
        }

        sw.stop();
        total_ms += sw.ms();

        // output is received from the drain device (by topology)  
        auto seg
            = receive_link // transfer the result back to host via receive_link
                .Receive(groute::Buffer<component_t>(&context.host_parents[0], context.nvtxs), groute::Event())
                .get();
        seg.Sync();
    }

    if (FLAGS_verbose) printf("\nPartitioning (CPU): %f ms.", par_total_ms / FLAGS_repetitions);
    printf("\nCC (Async): %f ms. <filter>\n\n", total_ms / FLAGS_repetitions);

    return CheckComponents(context.host_parents, context.nvtxs);
}
```
