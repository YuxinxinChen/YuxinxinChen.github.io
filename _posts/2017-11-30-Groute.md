
layout: post
title: "Notes on Groute"
data: 2017-11-30
tags: [reading notes, asynchronous, graph]
comments: true
share: false
---

### Overview about Groute Connect Components

Groute uses adaptive CC. On single machine, each thread takes an edge and does an atomic hook (some threads will compete to hook same node, then use atomic to ensure no overwriting happen. So this hook is called atomic hook). After that, we do a compression to flatten the tree into 1-level tree. Then the algorith ends. On multi-GPU, because remote atomic is expensive and sometimes, it is just simply infeasiable. So edges are partitioned and given to each GPU. Each GPU does local atomic hook and local compression regardless if some local connected components are disconnected because that linking edge is on other GPUs. After local hook and compresson are done, the CC information are reduced according to a policy ( one way: O(N), tree: O(logN) N is number of GPUs). The GPU/GPUs who reducing the CC information, because the received CC informaiton is also a 1-level tree, i.e. number of edges = number of nodes, then we can simply rerun the actomic hook and compression on received CC information, information is integrated.

One feature of Groute, which is also heavily used in Groute CC is its data transfer & compute pipeline. So the data are divided into chunks and each chunks are distributed in a round robin way to all the GPUs. Each GPU, after receiving the first chunk of data, it can start to do computation on that chunk of data without having to wait all the assigned data finishing transferring.The data tranfer and data computation can happen concurrently. GPU has two indepentant MMU to do data tranferring without interfering computation parts.

To support pipelining, there are several important data structure needed to understand. See below figure from a soul painter:
First Router is abstraction for communication and links is an abstraction connecting physical devices and routers. Then naive idea of the following structure used in CC is that: 
In input\_router, send\_link connects host and input\-router and is used for sending data, edges\_in[0]...edges\_in[N] connect input\_router and GPUs and is used for receiving data. 
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_input.png =250x250)
In reduction\_router, receive\_link connects host and reduction\_router for receiving data, reduction\_in[0]...reduction\_in[N] connects reduction\_router and GPUs for sending data. Here there is no link sending back from reduction\_router to host. So those reduction\_in[0]...reduction\_in[N] will be used to send data around GPUs and reduction\_out[0]...reduction\_out[N] connect reduction\_router for receiving data from other GPUs. So reduction\_router is basically used for communication between GPUs and send the final result to Host(CPU) when done.
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_reduct.png)

A detail data structure help understand. The link will construct a PipelinedSender or a PipelinedReceiver internally depending on the direction of the link. In PipelineSender constructs pipeline and m\_promise\_buff and m\_sender internally and m\_sender will stored under corresponding router. In PipelineReceiver, it constructs pipeline and m\_promise\_seg and m\_receiver internally and the m\_receiver will be stored under corresponding router. So all the links are constructed under input\_router and the PipelineSender will push send\_operation into corresponding PipelineReceivr's m\_receiver's m\_send\_queue just by asking the router for the destination address. PipelineReciever will push receive\_operation into its own m\_receiver's m\_receive\_queue. send\_op and receive\_op are paired when they are both got poped out and the corresponding memory copy work will be queue onto stream. When the memory copy work finishes, the call back function will update how much memory copy the send\_op has been done on send\_op so far and set the promise value of receive\_op. Then the computation who is waiting on that future gets unblocked. The red dash line in the below figure means the data structure can be access from router.
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_datastruct.png)
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_sendop.png)
So usually the send\_op is larger than receive\_op, there will be several receive\_op to consume one send\_op.

Mapping to Groute framework. Data are offloaded to GPU chunk by chunk via the sender and receiver. There is a flag: compute\_latency\_ratio, which define how much data are offloaded to GPU in advance before start computation. There is also an option to enable fragmentation transferring. The policy of router will determine how the set up network communicate. For input\_router, it broadcasts data to all GPUs and for reduction\_router, each GPU transfers its data to its level up boss. 

After GPUs have the data, it starts computation and then wait on next chunk's future untill consume all the data assigned. Then pipelined data transfer happens again from GPU to another GPU based on routing policy and the GPU who receives the data starts computation when it gets the first chunk of data. The reduction tree continues as all the information are integrated.

The main code of Groute is below:
```bash
bool RunCCMAsyncAtomic(int ngpus)
{
    cc::Context context(FLAGS_graphfile, FLAGS_ggr, FLAGS_verbose, ngpus); // Create context

    cc::Configuration configuration;  // Create configuration object and pass the configuration
    if (FLAGS_auto_config)
        cc::BuildConfigurationAuto( // Deduce the configuration automatically  
	.....                       // Configuration Code ignored
	)

    context.DisableFragmentation();  // The fragementation means the size of fragment of data when we transfer data, so even if the data transfer is pipelined, each chunk can be tansferred fragmently
    context.CacheEvents(
        std::max(configuration.input_pipeline_buffers, configuration.reduction_pipeline_buffers) /*raw estimation  */);

    double par_total_ms = 0.0, total_ms = 0.0; 

    for (size_t rep = 0; rep < FLAGS_repetitions; ++rep) // Define how many experiments wanna do 
    {
        Stopwatch psw(true);

        groute::Segment<Edge> all_edges = groute::Segment<Edge>(&context.host_edges[0], context.nedges, context.nedges, 0); // load edges into segment
        cc::EdgePartitioner partitioner(ngpus, context.nvtxs, all_edges, configuration.vertex_partitioning);  // Partition edges

        auto reduction_policy = FLAGS_tree_topology   // 
            ? groute::router::Policy::CreateTreeReductionPolicy(ngpus)
            : groute::router::Policy::CreateOneWayReductionPolicy(ngpus);

        groute::router::Router<Edge> input_router(context, std::make_shared<cc::EdgeScatterPolicy>(ngpus));
        groute::router::Router<int> reduction_router(context, reduction_policy);

        groute::router::ISender<Edge>* host_sender = input_router.GetSender(groute::Device::Host);
        groute::router::IReceiver<int>* host_receiver = reduction_router.GetReceiver(groute::Device::Host); // TODO

        IntervalRangeMarker iter_rng(context.nedges, "begin");

        for (auto& edge_partition : partitioner.edge_partitions)
        {
            host_sender->Send(edge_partition, groute::Event());
        }
        host_sender->Shutdown();

        psw.stop();
        par_total_ms += psw.ms();

        std::vector< std::unique_ptr<cc::Problem> > problems;
        std::vector< std::unique_ptr<cc::Solver> > solvers;
        std::vector<std::thread> workers(ngpus);

        dim3 block_dims(MASYNC_BS, 1, 1);

        for (size_t i = 0; i < ngpus; ++i)
        {
            problems.emplace_back(new cc::Problem(context, partitioner.parents_partitions[i], i, block_dims));
            solvers.emplace_back(new cc::Solver(context, *problems.back()));

            solvers[i]->edges_in = groute::Link<Edge>(input_router, i, configuration.edges_chunk_size, configuration.input_pipeline_buffers);

            solvers[i]->reduction_in = groute::Link<component_t>(reduction_router, i, configuration.parents_chunk_size, configuration.reduction_pipeline_buffers);
            solvers[i]->reduction_out = groute::Link<component_t>(i, reduction_router);
        }

        for (size_t i = 0; i < ngpus; ++i)
        {
            // Sync the first copy operations (exclude from timing)
            solvers[i]->edges_in.Sync();
        }

        groute::internal::Barrier barrier(ngpus + 1); // barrier for accurate timing  

        for (size_t i = 0; i < ngpus; ++i)
        {
            // Run workers  
            std::thread worker(
                [&configuration, &barrier](cc::Solver& solver)
            {
                barrier.Sync();
                barrier.Sync();
                solver.Solve(configuration);
            },
                std::ref(*solvers[i]));

            workers[i] = std::move(worker);
        }

        barrier.Sync();
        Stopwatch sw(true); // all threads are running, start timing
        barrier.Sync();

        for (size_t i = 0; i < ngpus; ++i)
        {
            // Join threads  
            workers[i].join();
        }

        sw.stop();
        total_ms += sw.ms();

        // output is received from the drain device (by topology)  
        auto seg
            = host_receiver
                ->Receive(groute::Buffer<int>(&context.host_parents[0], context.nvtxs), groute::Event())
                .get();
        seg.Sync();
    }

    if (FLAGS_verbose) printf("\nPartitioning (CPU): %f ms.", par_total_ms / FLAGS_repetitions);
    printf("\nCC (Async): %f ms. <filter>\n\n", total_ms / FLAGS_repetitions);

    return CheckComponents(context.host_parents, context.nvtxs);
}
```
