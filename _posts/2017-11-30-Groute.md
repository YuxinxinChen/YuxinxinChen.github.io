
layout: post
title: "Notes on Groute"
data: 2017-11-30
tags: [reading notes, asynchronous, graph]
comments: true
share: false
---

### Overview about Groute Connect Components

Groute uses adaptive CC. On single machine, each thread takes an edge and does an atomic hook (some threads will compete to hook same node, then use atomic to ensure no overwriting happen. So this hook is called atomic hook). After that, we do a compression to flatten the tree into 1-level tree. Then the algorith ends. On multi-GPU, because remote atomic is expensive and sometimes, it is just simply infeasiable. So edges are partitioned and given to each GPU. Each GPU does local atomic hook and local compression regardless if some local connected components are disconnected because that linking edge is on other GPUs. After local hook and compresson are done, the CC information are reduced according to a policy ( one way: O(N), tree: O(logN) N is number of GPUs). The GPU/GPUs who reducing the CC information, because the received CC informaiton is also a 1-level tree, i.e. number of edges = number of nodes, then we can simply rerun the actomic hook and compression on received CC information, information is integrated.

One feature of Groute, which is also heavily used in Groute CC is its data transfer & compute pipeline. So the data are divided into chunks and each chunks are distributed in a round robin way to all the GPUs. Each GPU, after receiving the first chunk of data, it can start to do computation on that chunk of data without having to wait all the assigned data finishing transferring.The data tranfer and data computation can happen concurrently. GPU has two indepentant MMU to do data tranferring without interfering computation parts.

To support pipelining, there are several important data structure needed to understand. See below figure from a soul painter:
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_datastruct.png)


Mapping to Groute framework. Data are offloaded to GPU chunk by chunk. There is a flag: compute\_latency\_ratio, which define how much data are offloaded to GPU in advance before start computation. There is also a
The main code of Groute is below:
```bash
bool RunCCMAsyncAtomic(int ngpus)
{
    cc::Context context(FLAGS_graphfile, FLAGS_ggr, FLAGS_verbose, ngpus); // Create context

    cc::Configuration configuration;  // Create configuration object and pass the configuration
    if (FLAGS_auto_config)
        cc::BuildConfigurationAuto( // Deduce the configuration automatically  
	.....                       // Configuration Code ignored
	)

    context.DisableFragmentation();  // The fragementation means the size of fragment of data when we transfer data, so even if the data transfer is pipelined, each chunk can be tansferred fragmently
    context.CacheEvents(
        std::max(configuration.input_pipeline_buffers, configuration.reduction_pipeline_buffers) /*raw estimation  */);

    double par_total_ms = 0.0, total_ms = 0.0; 

    for (size_t rep = 0; rep < FLAGS_repetitions; ++rep) // Define how many experiments wanna do 
    {
        Stopwatch psw(true);

        groute::Segment<Edge> all_edges = groute::Segment<Edge>(&context.host_edges[0], context.nedges, context.nedges, 0); // load edges into segment
        cc::EdgePartitioner partitioner(ngpus, context.nvtxs, all_edges, configuration.vertex_partitioning);  // Partition edges

        auto reduction_policy = FLAGS_tree_topology   // 
            ? groute::router::Policy::CreateTreeReductionPolicy(ngpus)
            : groute::router::Policy::CreateOneWayReductionPolicy(ngpus);

        groute::router::Router<Edge> input_router(context, std::make_shared<cc::EdgeScatterPolicy>(ngpus));
        groute::router::Router<int> reduction_router(context, reduction_policy);

        groute::router::ISender<Edge>* host_sender = input_router.GetSender(groute::Device::Host);
        groute::router::IReceiver<int>* host_receiver = reduction_router.GetReceiver(groute::Device::Host); // TODO

        IntervalRangeMarker iter_rng(context.nedges, "begin");

        for (auto& edge_partition : partitioner.edge_partitions)
        {
            host_sender->Send(edge_partition, groute::Event());
        }
        host_sender->Shutdown();

        psw.stop();
        par_total_ms += psw.ms();

        std::vector< std::unique_ptr<cc::Problem> > problems;
        std::vector< std::unique_ptr<cc::Solver> > solvers;
        std::vector<std::thread> workers(ngpus);

        dim3 block_dims(MASYNC_BS, 1, 1);

        for (size_t i = 0; i < ngpus; ++i)
        {
            problems.emplace_back(new cc::Problem(context, partitioner.parents_partitions[i], i, block_dims));
            solvers.emplace_back(new cc::Solver(context, *problems.back()));

            solvers[i]->edges_in = groute::Link<Edge>(input_router, i, configuration.edges_chunk_size, configuration.input_pipeline_buffers);

            solvers[i]->reduction_in = groute::Link<component_t>(reduction_router, i, configuration.parents_chunk_size, configuration.reduction_pipeline_buffers);
            solvers[i]->reduction_out = groute::Link<component_t>(i, reduction_router);
        }

        for (size_t i = 0; i < ngpus; ++i)
        {
            // Sync the first copy operations (exclude from timing)
            solvers[i]->edges_in.Sync();
        }

        groute::internal::Barrier barrier(ngpus + 1); // barrier for accurate timing  

        for (size_t i = 0; i < ngpus; ++i)
        {
            // Run workers  
            std::thread worker(
                [&configuration, &barrier](cc::Solver& solver)
            {
                barrier.Sync();
                barrier.Sync();
                solver.Solve(configuration);
            },
                std::ref(*solvers[i]));

            workers[i] = std::move(worker);
        }

        barrier.Sync();
        Stopwatch sw(true); // all threads are running, start timing
        barrier.Sync();

        for (size_t i = 0; i < ngpus; ++i)
        {
            // Join threads  
            workers[i].join();
        }

        sw.stop();
        total_ms += sw.ms();

        // output is received from the drain device (by topology)  
        auto seg
            = host_receiver
                ->Receive(groute::Buffer<int>(&context.host_parents[0], context.nvtxs), groute::Event())
                .get();
        seg.Sync();
    }

    if (FLAGS_verbose) printf("\nPartitioning (CPU): %f ms.", par_total_ms / FLAGS_repetitions);
    printf("\nCC (Async): %f ms. <filter>\n\n", total_ms / FLAGS_repetitions);

    return CheckComponents(context.host_parents, context.nvtxs);
}
```
