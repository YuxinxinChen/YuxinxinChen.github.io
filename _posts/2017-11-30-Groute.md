
layout: post
title: "Notes on Groute"
data: 2017-11-30
tags: [reading notes, asynchronous, graph]
comments: true
share: false
---

### Just some doodles

```c
class Event
{
    Event Record(cudaStream_t stream);
    void Wait(cudaStream_t stream) const = 0;
    void Sync() const = 0;
    bool Query() const = 0;
};

class EventGroup
{
    std::vector<Event> m_internal_events;
}
```
Event is a wrap of cudaEvent.

```c
class Segment
{
private:
    T* m_segment_ptr; // ptr of this segment
    size_t m_total_size; // total size of the total memory
    size_t m_segment_size; // size of this segment
    size_t m_segment_offset; // offset relative to the poiter of the total memory
}

class Pipeline
{
protected:
    size_t m_chunk_size; // size of each buffer
    std::vector <T*> m_endpoint_buffers; // buffer list
    Endpoint m_endpoint; // device ID where the buffer locates
    Context& m_ctx; // context
}

class ReceiveOperation
{
private:
    std::promise< PendingSegment<T> > m_promise;
    std::shared_future< PendingSegment<T> > m_shared_future;

    Endpoint m_src_endpoint;
    Endpoint m_dst_endpoint;

    Segment<T> m_src_segment; // The source is a segment of some valid data
    Buffer<T> m_dst_buffer; // The destination is any memory buffer with enough space
    Event m_dst_ready_event;

public:
    ReceiveOperation(Endpoint dst_endpoint, const Buffer<T>& dst_buffer, const Event& dst_ready_event) :
                m_src_endpoint(), m_dst_endpoint(dst_endpoint), m_dst_buffer(dst_buffer), m_dst_ready_event(dst_ready_event)
    {
    	m_shared_future = m_promise.get_future(); // get the future and (implicitly) cast to a shared future  
    }


    std::shared_future< PendingSegment<T> > GetFuture() const
    {
        return m_shared_future;
    }

    void SetSrcSegment(Endpoint src_endpoint, const Segment<T>& src_segment)
    {
    	m_src_endpoint = src_endpoint;
        m_src_segment = Segment<T>(src_segment);
    }

    void SetDstBuffer(Endpoint dst_endpoint, const Buffer<T>& dst_buffer)
    {
    	m_dst_endpoint = dst_endpoint;
        m_dst_buffer = Buffer<T>(dst_buffer);
    }


    Event GetDstReadyEvent() const
    {
    	return m_dst_ready_event;
    }

    void Complete(Event ready_event)
    {
    	m_promise.set_value(
        	PendingSegment<T>(
                m_dst_buffer.GetPtr(), m_src_segment.GetTotalSize(),
                m_src_segment.GetSegmentSize(), m_src_segment.GetSegmentOffset(),
                ready_event));
     }

     void Cancel()
     {
     	assert(!is_ready(m_shared_future));

        m_promise.set_value(PendingSegment<T>(m_dst_buffer.GetPtr(), 0, 0, 0, Event()));
      }
};
```

```c
class Context
    {
        //
        // The context provides an abstraction layer between virtual 'endpoints' and the actual physical devices in the system.
        // In addition, it provides global services, such as memory-copy lanes for queing asynchronous copy operations, and event management.
        //

        std::map<Endpoint, device_t> m_endpoint_map; // Maps from endpoints to physical devices   
        int m_fragment_size; // The fragment size determining memory copy granularity. 
                             // In some cases, such fragmentation improves responsiveness of the underlying node, by interleaving memory traffic   
        std::set<int> m_physical_devs; // The physical devices currently in use by this context  
        std::map<int, std::unique_ptr<EventPool> > m_event_pools;
        std::map<int, std::unique_ptr<MemoryPool> > m_memory_pools;
        std::map< LaneIdentifier, std::shared_ptr<IMemcpyInvoker> > m_memcpy_invokers; // Memory-copy workers for each lane
        mutable std::mutex m_mutex;
    }
```

![Gunrock](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/gunrock_cc.png)
![Groute](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_cc.png)

For Gunrock, we define:
```bash
t_grk is the time used in the grey block in above Gunrock diagram which is the max time across GPUs on local iterations of non-atomic hook and pointer jumping.
S is the time used for global barrier for synchronizing this outer iteration of non-atomic hook & pointr jumping and next outer iteration of non-atomic hook & pointer jumping. 
```
Then the runtime of Gunrock: T_grk = (t_grk + S)\*I

For Groute, we define:
```bash
t_grt is the time used to run Groute because basing on the belowe profiler, 90% time is used on the upper grey block in Groute diagram.
```

We identify

```bash
==27735== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   57.97%  147.50ms        89  1.6574ms  476.58us  1.7237ms  [CUDA memcpy HtoD]
                   34.01%  86.535ms        87  994.65us  725.06us  4.3243ms  void HookHighToLowAtomic<groute::graphs::dev::EdgeList>(groute::graphs::dev::Irregular<int>, groute::graphs::dev::EdgeList)
                    4.97%  12.637ms        88  143.60us  138.08us  254.05us  MultiJumpCompress(groute::graphs::dev::Irregular<int>)
                    2.54%  6.4532ms         3  2.1511ms  474.72us  5.5007ms  [CUDA memcpy DtoH]
                    0.32%  826.28us         2  413.14us  411.81us  414.47us  [CUDA memcpy PtoP]
                    0.13%  327.78us         4  81.945us  78.305us  85.473us  void HookHighToLowAtomic<groute::graphs::dev::Tree>(groute::graphs::dev::Irregular<int>, groute::graphs::dev::Tree)
                    0.06%  161.03us         3  53.675us  53.601us  53.761us  InitParents(groute::graphs::dev::Irregular<int>)
      API calls:   44.69%  1.05293s         6  175.49ms  11.839us  690.14ms  cudaDeviceEnablePeerAccess
                   21.79%  513.48ms         1  513.48ms  513.48ms  513.48ms  cudaHostAlloc
                    9.61%  226.55ms         1  226.55ms  226.55ms  226.55ms  cudaDeviceReset
                    9.33%  219.92ms         1  219.92ms  219.92ms  219.92ms  cudaFreeHost
                    6.60%  155.62ms        96  1.6211ms  252.57us  1.9354ms  cudaFree
                    5.92%  139.41ms        91  1.5319ms     817ns  31.979ms  cudaEventSynchronize
                    1.08%  25.439ms        96  264.99us  187.76us  656.50us  cudaMalloc
                    0.33%  7.8898ms        88  89.656us  5.4190us  7.2237ms  cudaMemcpyAsync
                    0.25%  5.8590ms       182  32.192us  23.429us  236.18us  cudaLaunch
                    0.17%  3.8931ms       376  10.354us     313ns  421.90us  cuDeviceGetAttribute
                    0.08%  1.9176ms         4  479.41us  465.05us  494.31us  cuDeviceTotalMem
                    0.03%  665.52us        98  6.7910us  3.5300us  113.35us  cudaStreamWaitEvent
                    0.02%  544.82us       189  2.8820us     916ns  42.352us  cudaEventRecord
                    0.02%  414.24us         4  103.56us  53.326us  178.47us  cudaMemcpyPeerAsync
                    0.01%  336.12us         4  84.028us  81.087us  91.647us  cuDeviceGetName
                    0.01%  300.18us         9  33.353us  13.681us  114.71us  cudaStreamCreateWithFlags
                    0.01%  246.49us       273     902ns     460ns  11.269us  cudaSetupArgument
                    0.01%  235.06us       182  1.2910us  1.0170us  5.1500us  cudaConfigureCall
                    0.01%  210.40us       148  1.4210us     426ns  40.658us  cudaSetDevice
                    0.01%  151.00us       119  1.2680us     309ns  27.273us  cudaGetDevice
                    0.00%  108.40us         9  12.044us  2.8790us  37.142us  cudaStreamDestroy
                    0.00%  97.663us        99     986ns     463ns  12.112us  cudaEventCreateWithFlags
                    0.00%  68.033us        99     687ns     425ns  3.2440us  cudaEventDestroy
                    0.00%  12.620us         1  12.620us  12.620us  12.620us  cudaGetDeviceCount
                    0.00%  6.3500us         8     793ns     360ns  1.6030us  cuDeviceGet
                    0.00%  4.9600us         3  1.6530us     391ns  3.3520us  cuDeviceGetCount
                    0.00%  1.7060us         1  1.7060us  1.7060us  1.7060us  cuInit
```
