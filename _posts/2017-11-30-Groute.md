
layout: post
title: "Notes on Groute"
data: 2017-11-30
tags: [reading notes, asynchronous, graph]
comments: true
share: false
---

### Overview about Groute Connect Components

Groute uses adaptive CC. On single machine, each thread takes an edge and does an atomic hook (some threads will compete to hook same node, then use atomic to ensure no overwriting happen. So this hook is called atomic hook). After that, we do a compression to flatten the tree into 1-level tree. Then the algorith ends. On multi-GPU, because remote atomic is expensive and sometimes, it is just simply infeasiable. So edges are partitioned and given to each GPU. Each GPU does local atomic hook and local compression regardless if some local connected components are disconnected because that linking edge is on other GPUs. After local hook and compresson are done, the CC information are reduced according to a policy ( one way: O(N), tree: O(logN) N is number of GPUs). The GPU/GPUs who reducing the CC information, because the received CC informaiton is also a 1-level tree, i.e. number of edges = number of nodes, then we can simply rerun the actomic hook and compression on received CC information, information is integrated.

One feature of Groute, which is also heavily used in Groute CC is its data transfer & compute pipeline. So the data are divided into chunks and each chunks are distributed in a round robin way to all the GPUs. Each GPU, after receiving the first chunk of data, it can start to do computation on that chunk of data without having to wait all the assigned data finishing transferring.The data tranfer and data computation can happen concurrently. GPU has two indepentant MMU to do data tranferring without interfering computation parts.

To support pipelining, there are several important data structure needed to understand. See below figure from a soul painter:
First Router is abstraction for communication and links is an abstraction connecting physical devices and routers. Then naive idea of the following structure used in CC is that: 
In input\_router, send\_link connects host and input\-router and is used for sending data, edges\_in[0]...edges\_in[N] connect input\_router and GPUs and is used for receiving data. 
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_input.png)
In reduction\_router, receive\_link connects host and reduction\_router for receiving data, reduction\_in[0]...reduction\_in[N] connects reduction\_router and GPUs for sending data. Here there is no link sending back from reduction\_router to host. So those reduction\_in[0]...reduction\_in[N] will be used to send data around GPUs and reduction\_out[0]...reduction\_out[N] connect reduction\_router for receiving data from other GPUs. So reduction\_router is basically used for communication between GPUs and send the final result to Host(CPU) when done.
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_reduct.png)
The links are attached to its router. Different routers are different system and has its own links like blood vessel sending blood to parts of body.

A detail data structure help understand. The link will construct a PipelinedSender or a PipelinedReceiver internally depending on the direction of the link. In PipelineSender constructs pipeline and m\_promise\_buff and m\_sender internally and m\_sender will stored under corresponding router. In PipelineReceiver, it constructs pipeline and m\_promise\_seg and m\_receiver internally and the m\_receiver will be stored under corresponding router. So all the links are constructed under input\_router and the PipelineSender will push send\_operation into corresponding PipelineReceivr's m\_receiver's m\_send\_queue just by asking the router for the destination address. PipelineReciever will push receive\_operation into its own m\_receiver's m\_receive\_queue. send\_op and receive\_op are paired when they are both got poped out and the corresponding memory copy work will be queue onto stream. When the memory copy work finishes, the call back function will update how much memory copy the send\_op has been done on send\_op so far and set the promise value of receive\_op. Then the computation who is waiting on that future gets unblocked. The red dash line in the below figure means the data structure can be access from router.
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_datastruct.png)
![](https://github.com/YuxinxinChen/YuxinxinChen.github.io/blob/master/images/groute_sendop.png)
So usually the send\_op is larger than receive\_op, there will be several receive\_op to consume one send\_op.

Mapping to Groute framework. Data are offloaded to GPU chunk by chunk via the sender and receiver. There is a flag: compute\_latency\_ratio, which define how much data are offloaded to GPU in advance before start computation. There is also an option to enable fragmentation transferring. The policy of router will determine how the set up network communicate. For input\_router, it broadcasts data to all GPUs and for reduction\_router, each GPU transfers its data to its level up boss. 

After GPUs have the data, it starts computation and then wait on next chunk's future untill consume all the data assigned. Then pipelined data transfer happens again from GPU to another GPU based on routing policy and the GPU who receives the data starts computation when it gets the first chunk of data. The reduction tree continues as all the information are integrated.

The main code of Groute is below:
```bash
bool RunCCMAsyncAtomic(int ngpus)
{
    cc::Context context(FLAGS_graphfile, FLAGS_ggr, FLAGS_verbose, ngpus); // Create context

    cc::Configuration configuration;  // Create configuration object and pass the configuration
    cc::BuildConfigurationAuto( // Deduce the configuration automatically  
    .....)                       // Configuration Code ignored

    context.DisableFragmentation();  // The fragementation means the size of fragment of data when we transfer data, so even if the data transfer is pipelined, each chunk can be tansferred fragmently
    
    double par_total_ms = 0.0, total_ms = 0.0; 
    for (size_t rep = 0; rep < FLAGS_repetitions; ++rep)
    {
        Stopwatch psw(true);

        groute::Segment<Edge> all_edges = groute::Segment<Edge>(&context.host_edges[0], context.nedges, context.nedges, 0); // Save all edges into Segment object
        cc::EdgePartitioner partitioner(ngpus, context.nvtxs, all_edges, configuration.vertex_partitioning); // Partition the edges into several Segments

        psw.stop(); // Partitioning time
        par_total_ms += psw.ms();

        auto reduction_policy = FLAGS_tree_topology // Define reduction policy
            ? groute::Policy::CreateTreeReductionPolicy(ngpus)
            : groute::Policy::CreateOneWayReductionPolicy(ngpus);

        groute::Router<Edge> input_router(context, (std::shared_ptr<groute::IPolicy>)std::make_shared<cc::EdgeScatterPolicy>(ngpus), 1, ngpus); // Create input router
        groute::Router<component_t> reduction_router(context, reduction_policy, ngpus, ngpus+1); // Create reduction router

        groute::Endpoint host = groute::Endpoint::HostEndpoint(0); // Int ID of Host, usually 0
        groute::Link<Edge> send_link(host, input_router); // Create link from host to input_router
        groute::Link<component_t> receive_link(reduction_router, host, 0, 0); // Create link from reduction_router to host no pipelining here 

        std::vector< std::unique_ptr<cc::Problem> > problems; // Create problem object which specify what operation the algorithm CC has and its corresponding data structure
        std::vector< std::unique_ptr<cc::Solver> > solvers; // Create solver object which construct the algorithm using the block operation in problem object
        std::vector<std::thread> workers(ngpus); // Each worker thread will transfer data and start the computation to each GPU

        dim3 block_dims(MASYNC_BS, 1, 1);

        for (int i = 0; i < ngpus; ++i) // Each GPU has its problem and solver and links
        {
            problems.emplace_back(new cc::Problem(context, partitioner.parents_partitions[i], i, block_dims)); // Each GPU has it is own problem defined by the data assigned
            solvers.emplace_back(new cc::Solver(context, *problems.back())); // Solver per problem

            solvers[i]->edges_in = groute::Link<Edge>(input_router, i, configuration.edges_chunk_size, configuration.input_pipeline_buffers); 
	    ///////////////////////////////////////////////////////////////////////
            // edges_in link per GPU to get data from input_router. The link will//
            // link will construct a pipelinedReceiver which in itsconstruction, //
            // number of input_pipeline_buffers of receive_op are pushed onto its//
            // m_receiver's m_queue_receive and receive_op's future are pushe    //
            // onto m_promise_seg.                                               //
            // input_pipeline_buffers is computed from compute_latency_ratio     //
	    // which determine how much data are offloaded to the GPU in advance //
	    // Note if this ratio=0, all data are transferred on GPUs before     //
            // computation begins                                                //
            ///////////////////////////////////////////////////////////////////////

            solvers[i]->reduction_in = groute::Link<component_t>(reduction_router, i, configuration.parents_chunk_size, configuration.reduction_pipeline_buffers); // reduction_in link to reduction_router to get data from other GPU since every GPU also has a reduction_out link to reduction_router
            solvers[i]->reduction_out = groute::Link<component_t>(i, reduction_router); // reduction_out link to reduction_router to send data to other GPU since every GPU also has a reduction_in link to reduction_router
        }

        IntervalRangeMarker iter_rng(context.nedges, "begin");

        for (auto& edge_partition : partitioner.edge_partitions) // for each partition, clikc send buttom
        {
            send_link.Send(edge_partition, groute::Event()); 
	    ///////////////////////////////////////////////////////////////////////
            // send_link.Send call m_sender.send:                                //
            // 1. Create EventGroupPromises                                      //
            // 2. Create send_op                                                 //
            // 3. queue send_op in that partition target device's m_send_queue   //
            // 4. call partition target device's m_receiver's Assigne()--------> //
	    // pair a receive_op with a send_op and pop the receive_op, reduce   //
	    // the send_op size (pop send_op when all send_op is consumed) then  // 
            // call input_router.queueMemcpyWork(receive_op,send_op)------> when //
            // the copywork finish, a call back of queueMemcpyWork call:         //
            // receive_op.complete()                                             //
            // send_op.reportProgress(size)                                      //
            // Though the mem copy work hasn't got chance, the code continue to  //
            // execute since it is unblocking, somewhere later in the code who   //
            // call get() on receive_op's future may block on that since the copy//
            // work hasn't finished yet                                          // 
	    ///////////////////////////////////////////////////////////////////////
        }
        send_link.Shutdown();

        for (int i = 0; i < ngpus; ++i)
        {
            // Sync the first pipeline copy operations (exclude from timing)
            solvers[i]->edges_in.PipelineSync();
	    // For each GPU, number of input_pipeline_buffer chunks of data finish transfering data to GPU
        }

        groute::internal::Barrier barrier(ngpus + 1); // barrier for accurate timing  

        for (int i = 0; i < ngpus; ++i)
        {
            // Run workers, each one launchs work on each GPU
            std::thread worker(
                [&configuration, &barrier](cc::Solver& solver)
            {
                barrier.Sync();
                barrier.Sync();
                solver.Solve(configuration); // start the CC algorithm, unblocking
            },
                std::ref(*solvers[i]));

            workers[i] = std::move(worker);
        }

        barrier.Sync();     // sync for accurate start timing 
        Stopwatch sw(true); // all threads are running, start timing  
        barrier.Sync();     // and signal

        for (int i = 0; i < ngpus; ++i)
        {
            // Join threads  
            workers[i].join();  //threads will join here when solver.solve finishs
        }

        sw.stop();
        total_ms += sw.ms();

        // output is received from the drain device (by topology)  
        auto seg
            = receive_link // transfer the result back to host via receive_link
                .Receive(groute::Buffer<component_t>(&context.host_parents[0], context.nvtxs), groute::Event())
                .get();
        seg.Sync();
    }

    if (FLAGS_verbose) printf("\nPartitioning (CPU): %f ms.", par_total_ms / FLAGS_repetitions);
    printf("\nCC (Async): %f ms. <filter>\n\n", total_ms / FLAGS_repetitions);

    return CheckComponents(context.host_parents, context.nvtxs);
}
```
CC problem and solver data structure:
```bash
namespace cc
{
    struct Problem // a per device cc problem
    {
        const Context& context;

        groute::Endpoint endpoint; // Usually each GPU has own problem, endpoint indicate this problem is on which device
        dim3 block_dims;
        size_t compressed_size;

        Partition partition;  // In CC, edges are divided to distribute to every GPU. Each GPU keeps a hash table to store CC information for all vertices
        groute::Segment<int> parents; // Hash table to store CC infor

        cudaStream_t    compute_stream; // compute_stream to sync the sequence of code execution
        cudaEvent_t     sync_event;

        Problem(const Problem& other) = delete;
        Problem(Problem&& other) = delete;

        Problem(
            const Context& context, 
            const Partition& partition, groute::Endpoint endpoint, dim3 block_dims) :
            context(context), partition(partition), 
            endpoint(endpoint), block_dims(block_dims), compressed_size(0)
        {
            size_t parents_ss; // The parent segment size
            size_t parents_so; // The parent segment offset

            parents_ss = partition.parents_segment.GetSegmentSize();
            parents_so = partition.parents_segment.GetSegmentOffset();

            int* parents_ptr;

            context.SetDevice(endpoint);
            GROUTE_CUDA_CHECK(cudaMalloc(&parents_ptr, parents_ss * sizeof(int)));
            GROUTE_CUDA_CHECK(cudaStreamCreateWithFlags(&compute_stream, cudaStreamNonBlocking));
            GROUTE_CUDA_CHECK(cudaEventCreateWithFlags(&sync_event, cudaEventDisableTiming));

            parents = groute::Segment<int>(parents_ptr, context.nvtxs, parents_ss, parents_so); // Create CC infor hash table segment
        }

        ~Problem()
        {
            context.SetDevice(endpoint);
            GROUTE_CUDA_CHECK(cudaFree(parents.GetSegmentPtr()));
            GROUTE_CUDA_CHECK(cudaStreamDestroy(compute_stream));
            GROUTE_CUDA_CHECK(cudaEventDestroy(sync_event));
        }

        void Init() const
        {
            dim3 grid_dims(round_up(parents.GetSegmentSize(), block_dims.x), 1, 1);

            InitParents <<< grid_dims, block_dims, 0, compute_stream >>> (
                groute::graphs::dev::Irregular<int>(parents.GetSegmentPtr(), parents.GetSegmentSize(), parents.GetSegmentOffset())); // init each vertex's cc to itself
        }

        template<bool R1 = false>
        void Work(const groute::PendingSegment<Edge>& edge_seg) //non-atomic hook
        {
            dim3 grid_dims(round_up(edge_seg.GetSegmentSize(), block_dims.x), 1, 1);

            edge_seg.Wait(compute_stream); // queue a wait on the compute stream  

            Marker::MarkWorkitems(edge_seg.GetSegmentSize(), "Hook");

            Hook <groute::graphs::dev::EdgeList, R1> <<< grid_dims, block_dims, 0, compute_stream >>>(
                groute::graphs::dev::Irregular<int>(parents.GetSegmentPtr(), parents.GetSegmentSize(), parents.GetSegmentOffset()),
                groute::graphs::dev::EdgeList(edge_seg.GetSegmentPtr(), edge_seg.GetSegmentSize())
                );
            compressed_size = 0;
        }

        void WorkAtomic(const groute::PendingSegment<Edge>& edge_seg)  //atomic hook
        {
            dim3 grid_dims(round_up(edge_seg.GetSegmentSize(), block_dims.x), 1, 1);
            
            edge_seg.Wait(compute_stream); // queue a wait on the compute stream  

            Marker::MarkWorkitems(edge_seg.GetSegmentSize(), "HookHighToLowAtomic");

            HookHighToLowAtomic <groute::graphs::dev::EdgeList> <<< grid_dims, block_dims, 0, compute_stream >>>(
                groute::graphs::dev::Irregular<int>(parents.GetSegmentPtr(), parents.GetSegmentSize(), parents.GetSegmentOffset()),
                groute::graphs::dev::EdgeList(edge_seg.GetSegmentPtr(), edge_seg.GetSegmentSize())
                );
            compressed_size = 0;
        }

        void Merge(const groute::PendingSegment<int>& merge_seg)  //Since the 1-level tree transferred from other GPU is also interpreted as a bulk of edges, atomic-hook those edges
        {
            assert(merge_seg.GetSegmentOffset() >= parents.GetSegmentOffset());
            assert(merge_seg.GetSegmentOffset() - parents.GetSegmentOffset() + 
                merge_seg.GetSegmentSize() <= parents.GetSegmentSize()); 

            dim3 grid_dims(round_up(merge_seg.GetSegmentSize(), block_dims.x), 1, 1);

            merge_seg.Wait(compute_stream); // queue a wait on the compute stream  

            Marker::MarkWorkitems(merge_seg.GetSegmentSize(), "HookHighToLowAtomic");

            // Note the Tree graph and the vertex grid dims
            HookHighToLowAtomic <groute::graphs::dev::Tree> <<< grid_dims, block_dims, 0, compute_stream >>> (
                groute::graphs::dev::Irregular<int>(parents.GetSegmentPtr(), parents.GetSegmentSize(), parents.GetSegmentOffset()),
                groute::graphs::dev::Tree(merge_seg.GetSegmentPtr(), merge_seg.GetSegmentSize(), merge_seg.GetSegmentOffset())
                );
            compressed_size = 0;
        }

        void Compress() // MultiPointerJumping
        {
            dim3 grid_dims(round_up(parents.GetSegmentSize(), block_dims.x), 1, 1);

            Marker::MarkWorkitems(parents.GetSegmentSize(), "MultiJumpCompress");

            MultiJumpCompress <<< grid_dims, block_dims, 0, compute_stream >>> (
                groute::graphs::dev::Irregular<int>(parents.GetSegmentPtr(), parents.GetSegmentSize(), parents.GetSegmentOffset()));

            compressed_size = parents.GetSegmentSize(); // keep the compress size 
        }

        void CompressLocal()
        {
            size_t size = partition.local_upper_bound - parents.GetSegmentOffset();

            assert(size <= parents.GetSegmentSize());
            assert(size > 0);

            dim3 grid_dims(round_up(size, block_dims.x), 1, 1);

            Marker::MarkWorkitems(size, "MultiJumpCompress");


            MultiJumpCompress <<< grid_dims, block_dims, 0, compute_stream >>> (
                groute::graphs::dev::Irregular<int>(parents.GetSegmentPtr(), size, parents.GetSegmentOffset()));

            compressed_size = size; // keep the compress size 
        }

        void Compress(const groute::PendingSegment<int>& merged_seg)
        {
            // makes the decision what part to compress after a merge  
            // compresses from begining of local parents and up to covering the merged segment  

            assert(merged_seg.GetSegmentOffset() >= parents.GetSegmentOffset());

            size_t offset_diff = merged_seg.GetSegmentOffset() - parents.GetSegmentOffset();
            size_t size = offset_diff + merged_seg.GetSegmentSize();

            assert(size <= parents.GetSegmentSize());

            dim3 grid_dims(round_up(size, block_dims.x), 1, 1);

            Marker::MarkWorkitems(size, "MultiJumpCompress");


            MultiJumpCompress <<< grid_dims, block_dims, 0, compute_stream >>> (
                groute::graphs::dev::Irregular<int>(parents.GetSegmentPtr(), size, parents.GetSegmentOffset()));

            compressed_size = size; // keep the compress size 
        }

        void Finish()
        {
            if (compressed_size == parents.GetSegmentSize()) return;
            Compress(); // make sure the entire local segment is compressed   
        }

        groute::Event Record() const
        {
            return context.RecordEvent(endpoint, compute_stream);
        }

        void Sync() const
        {
            GROUTE_CUDA_CHECK(cudaEventRecord(sync_event, compute_stream));
            GROUTE_CUDA_CHECK(cudaEventSynchronize(sync_event));
        }
    };

    struct Solver // a per device cc solver  
    {
        const Context& context;
        Problem& problem;

        groute::Link<Edge> edges_in;
        groute::Link<component_t> reduction_in;
        groute::Link<component_t> reduction_out;

        Solver(const Solver& other) = delete;
        Solver(Solver&& other) = delete;

        Solver(const Context& context, Problem& problem) :
            context(context), problem(problem)
        {
        }

        void Solve(const Configuration& configuration)
        {
            context.SetDevice(problem.endpoint);

            // Init
            problem.Init();
            
            auto input_fut = edges_in.PipelinedReceive();
            auto reduce_fut = reduction_in.PipelinedReceive();

            groute::PendingSegment<int> merge_seg;
            groute::PendingSegment<Edge> input_seg;

            for (int i = 0;; ++i) // Do atomic hook and compression on chunks of data assigned sequentially locally
            {
                input_seg = input_fut.get(); //Wait on a chunk of data to process
                if (input_seg.Empty()) break; // When there is no input data, the localCC finishs

                for (int ii = 0; ii < configuration.nonatomic_rounds; ++ii) // non atomic rounds, only is called for low dense graph, since non atomic hook won't lose much infor. 
                {
                    IntervalRangeMarker iter_rng(input_seg.GetSegmentSize(), "CC iteration (non-atomic)");

                    if (i == 0 && ii == 0)  problem.Work<true>(input_seg); // round 1
                    else                    problem.Work<false>(input_seg);

                    problem.CompressLocal(); 
                        // for non atomic rounds we must compress any how  
                        // compress only up to local bounds, everything else will get compressed later   
                }

                IntervalRangeMarker iter_rng(input_seg.GetSegmentSize(), "CC iteration (atomic)");


                // Work atomic
                problem.WorkAtomic(input_seg); // Do atomic hook on chunk of data, process a serise of chunk of data sequencially by the outer loop

                edges_in.ReleaseReceiveBuffer(input_seg.GetSegmentPtr(), problem.Record()); 
	        // dismiss depends on the recorded event, release receive buffer and push a new receive_op in m_queue_receive and queue a future in m_promised_seg
                input_fut = edges_in.PipelinedReceive();
		// Pop the next future in m_promised_seg, later to wait on.

                problem.Compress();  // Do MultiPointerJumping 

                // Sync CPU thread every odd segment to obtain   
                // dynamic load balancing for input segments    
                // NOTE: figure out a better approach  
                if(configuration.load_balancing && (i%2) == 1) problem.Sync(); 
            }

            for (int i = 0;; ++i) // The GPU who integrate CC infor in reduction tree style will block at reduce_fut.get. GPUs who just need to send CC infor won't enter this loop
            {
                merge_seg = reduce_fut.get(); // Get CC infor from other GPU
                if (merge_seg.Empty()) break; // No input CC infor, stop
                IntervalRangeMarker iter_rng(merge_seg.GetSegmentSize(), "CC merge iteration");


                // Merge
                problem.Merge(merge_seg); // Run Merge on received chunk of data

                reduction_in.ReleaseReceiveBuffer(merge_seg.GetSegmentPtr(), problem.Record()); 
	        // dismiss depends on the recorded event, push new receive_op on m_queue_receive and its future on m_promised_seg
                reduce_fut = reduction_in.PipelinedReceive(); // Pop next future in m_promised_seg
            }

            // Makes sure the entire local segment is compressed  
            problem.Finish();

            // Send the final distribution to peers
            reduction_out.Send(problem.parents, problem.Record()); // GPUs who send CC infor click on send to send to its upper level boss, exit when data copy finishs. Its boss who wait on the data will unblock from the future.get()
            reduction_out.Shutdown();

            problem.Sync();
        }
    };
}
```
